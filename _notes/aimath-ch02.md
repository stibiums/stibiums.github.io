---
layout: post
title: "AI数学基础 - 第2讲: 条件概率与独立性"
date: 2025-09-13 02:00:00
tags: notes aimath probability
categories: aimath
toc:
  sidebar: left
---

## 2.1 条件概率

### 引入问题

在已知某些信息的条件下，如何计算事件发生的概率？这就是条件概率要解决的问题。

**实例**：已知掷骰子的结果是偶数，求结果为2的概率。

### 条件概率的定义

设 $A$、$B$ 是两个事件，且 $P(B) > 0$，则称
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$
为在事件 $B$ 发生的条件下事件 $A$ 发生的**条件概率**。

### 条件概率的理解

1. **缩减样本空间**：条件 $B$ 的给出使得样本空间从 $\Omega$ 缩减为 $B$
2. **重新分配概率**：在新的样本空间 $B$ 中，各事件的概率按比例重新分配
3. **归一化处理**：除以 $P(B)$ 使得新概率满足规范性

### 条件概率的性质

条件概率 $P(\cdot \mid B)$ 仍然是概率，满足概率的所有公理：

1. **非负性**：$P(A \mid B) \geq 0$
2. **规范性**：$P(\Omega \mid B) = 1$
3. **可加性**：若 $A_1$、$A_2$ 互不相容，则
   $$P(A_1 \cup A_2 \mid B) = P(A_1 \mid B) + P(A_2 \mid B)$$

### 乘法公式

由条件概率的定义可得：
$$P(A \cap B) = P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)$$

**一般化的乘法公式**：
$$P(A_1 \cap A_2 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2 \mid A_1) \cdot P(A_3 \mid A_1 \cap A_2) \cdots P(A_n \mid A_1 \cap \cdots \cap A_{n-1})$$

### 例题

**例1**：一个家庭有两个孩子，已知至少有一个是男孩，求两个都是男孩的概率。

**解**：

- 设 $A$ = "两个都是男孩"，$B$ = "至少有一个男孩"
- $\Omega = \{($男,男$), ($男,女$), ($女,男$), ($女,女$)\}$
- $A = \{($男,男$)\}$，$B = \{($男,男$), ($男,女$), ($女,男$)\}$
- $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{1/4}{3/4} = \frac{1}{3}$

## 2.2 全概率公式

### 样本空间的划分

设 $B_1, B_2, \ldots, B_n$ 是样本空间 $\Omega$ 的一个**划分**，即：

1. **互不相交**：$B_i \cap B_j = \emptyset$ $(i \neq j)$
2. **穷尽性**：$\bigcup_{i=1}^{n} B_i = \Omega$
3. **非空性**：$P(B_i) > 0$ $(i = 1, 2, \ldots, n)$

### 全概率公式

对于任意事件 $A$，有：
$$P(A) = \sum_{i=1}^{n} P(A \mid B_i) \cdot P(B_i)$$

### 全概率公式的应用

**应用场景**：

- 当直接求 $P(A)$ 困难时
- 可以按照某种条件将样本空间划分
- 在每个子条件下计算 $P(A)$ 相对容易

**例题：次品检测**
某工厂有三条生产线，产量比例为 2:3:5，次品率分别为 1%、2%、1.5%。随机取一件产品，求该产品是次品的概率。

**解**：
设 $B_i$ 表示产品来自第 $i$ 条生产线，$A$ 表示产品是次品

- $P(B_1) = 0.2$，$P(B_2) = 0.3$，$P(B_3) = 0.5$
- $P(A \mid B_1) = 0.01$，$P(A \mid B_2) = 0.02$，$P(A \mid B_3) = 0.015$

由全概率公式：
$$P(A) = 0.01 \times 0.2 + 0.02 \times 0.3 + 0.015 \times 0.5 = 0.0155$$

## 2.3 贝叶斯公式

### 公式表述

设 $B_1, B_2, \ldots, B_n$ 是样本空间 $\Omega$ 的一个划分，$A$ 是任一事件且 $P(A) > 0$，则：
$$P(B_i \mid A) = \frac{P(A \mid B_i) \cdot P(B_i)}{\sum_{j=1}^{n} P(A \mid B_j) \cdot P(B_j)}$$

### 术语解释

- **$P(B_i)$**：**先验概率** (Prior Probability)
  - 在观察到结果 $A$ 之前，对原因 $B_i$ 的概率估计
- **$P(A \mid B_i)$**：**似然度** (Likelihood)
  - 在原因 $B_i$ 发生的条件下，观察到结果 $A$ 的概率
- **$P(B_i \mid A)$**：**后验概率** (Posterior Probability)
  - 在观察到结果 $A$ 之后，原因是 $B_i$ 的概率

### 贝叶斯公式的意义

1. **因果推理**：从结果推原因
2. **信息更新**：利用新信息更新概率认识
3. **学习机制**：是机器学习的重要理论基础

### 经典应用：医学诊断

**问题**：某种疾病的患病率为 0.1%，有一种检测方法：

- 患病者检测呈阳性的概率为 99%（敏感性）
- 健康者检测呈阳性的概率为 2%（假阳性率）

求检测呈阳性的人确实患病的概率。

**解**：
设 $D$ 表示患病，$T^+$ 表示检测呈阳性

- $P(D) = 0.001, P(D^c) = 0.999$
- $P(T^+ \mid D) = 0.99, P(T^+ \mid D^c) = 0.02$

由贝叶斯公式：
$$P(D \mid T^+) = \frac{P(T^+ \mid D) \cdot P(D)}{P(T^+ \mid D) \cdot P(D) + P(T^+ \mid D^c) \cdot P(D^c)}$$
$$= \frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.02 \times 0.999} = \frac{0.00099}{0.00099 + 0.01998} \approx 0.047$$

**结论**：即使检测呈阳性，患病概率仅约为 4.7%！这说明了先验概率的重要性。

## 2.4 事件的独立性

### 两个事件的独立性

#### 定义

事件 $A$ 和 $B$ **相互独立**，当且仅当：
$$P(A \cap B) = P(A) \cdot P(B)$$

#### 等价条件

当 $P(B) > 0$ 时，$A$ 与 $B$ 独立等价于：

- $P(A \mid B) = P(A)$
- $P(B \mid A) = P(B)$ （当 $P(A) > 0$ 时）

#### 独立性的理解

- **信息无关**：$B$ 的发生不影响 $A$ 发生的概率
- **统计无关**：两个事件没有统计关联
- **乘法规则**：独立事件的联合概率等于边际概率的乘积

### 重要性质

1. 若 $A$ 与 $B$ 独立，则：

   - $A^c$ 与 $B$ 独立
   - $A$ 与 $B^c$ 独立
   - $A^c$ 与 $B^c$ 独立

2. 若 $P(A) = 0$ 或 $P(A) = 1$，则 $A$ 与任何事件 $B$ 都独立

3. 不可能事件与必然事件与任何事件都独立

### 多个事件的独立性

#### 两两独立

事件 $A_1, A_2, \ldots, A_n$ **两两独立**，当且仅当其中任意两个事件都相互独立：
$$P(A_i \cap A_j) = P(A_i) \cdot P(A_j), \quad i \neq j$$

#### 相互独立

事件 $A_1, A_2, \ldots, A_n$ **相互独立**，当且仅当对于任意的子集 $\{i_1, i_2, \ldots, i_k\} \subseteq \{1, 2, \ldots, n\}$，都有：
$$P(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_k}) = P(A_{i_1}) \cdot P(A_{i_2}) \cdots P(A_{i_k})$$

#### 关系

- **相互独立 $\Rightarrow$ 两两独立**
- **两两独立 $\nRightarrow$ 相互独立**

**反例**：考虑三个事件的情况，构造一个两两独立但不相互独立的例子。

## 2.5 独立试验

### 伯努利试验

**定义**：只有两个可能结果的试验：

- "成功"，概率为 $p$
- "失败"，概率为 $q = 1-p$

### n 重伯努利试验

进行 $n$ 次独立的伯努利试验，其中：

- 每次试验成功的概率都是 $p$
- 各次试验相互独立

**二项分布**：$n$ 次试验中恰好成功 $k$ 次的概率为：
$$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$$

记作 $X \sim B(n, p)$

### 应用实例

**例**：某射手命中率为 0.8，独立射击 5 次，求：

1. 恰好命中 3 次的概率
2. 至少命中 3 次的概率

**解**：

1. $P(X = 3) = \binom{5}{3} \times 0.8^3 \times 0.2^2 = 10 \times 0.512 \times 0.04 = 0.2048$

2. $P(X \geq 3) = P(X = 3) + P(X = 4) + P(X = 5)$
   = 0.2048 + 0.4096 + 0.32768 = 0.94208

## 2.6 随机游走简介

### 一维随机游走

考虑一个质点在整数轴上的运动：

- 初始位置在原点 (0)
- 每一步向右移动一个单位的概率为 p
- 每一步向左移动一个单位的概率为 q = 1-p

设 $S_n$ 表示 n 步后质点的位置：
$$S_n = X_1 + X_2 + \cdots + X_n$$
其中 $X_i$ 是第 i 步的位移（+1 或 -1）。

### 对称随机游走

当 p = q = 1/2 时，称为**对称随机游走**。

**性质**：

- $E[S_n] = 0$（期望回到原点）
- $\text{Var}(S_n) = n$（方差随时间线性增长）
- 具有马尔可夫性质

### 首次到达问题

**问题**：从原点出发，首次到达位置 a (a > 0) 的概率是多少？

设 $p_a$ 为从原点出发首次到达 a 的概率。通过马尔可夫性质，可以建立递推关系：

$$p_a = p \cdot p_{a-1} + q \cdot p_{a+1}$$

**边界条件**：$p_0 = 1$

**解**：

- 当 $p = q = 1/2$ 时：$p_a = 1$
- 当 $p \neq q$ 时：
  - 若 $p > q$：$p_a = (q/p)^a$
  - 若 $p < q$：$p_a = 1$

### 随机游走的应用

1. **金融数学**：股价模型、期权定价
2. **物理学**：布朗运动、扩散过程
3. **生物学**：分子运动、基因漂移
4. **计算机科学**：算法分析、网络分析
5. **机器学习**：马尔可夫链蒙特卡洛方法

## 小结

### 本讲核心内容

1. **条件概率**：$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$

   - 体现了信息对概率认识的影响
   - 是贝叶斯方法的基础

2. **全概率公式**：$P(A) = \sum P(A \mid B_i)P(B_i)$

   - 提供了计算复杂事件概率的有效方法
   - 体现了"分而治之"的思想

3. **贝叶斯公式**：$P(B_i \mid A) = \frac{P(A \mid B_i)P(B_i)}{P(A)}$

   - 从结果推原因的重要工具
   - 机器学习中贝叶斯方法的理论基础

4. **独立性**：$P(A \cap B) = P(A) \cdot P(B)$
   - 简化概率计算的重要性质
   - 独立试验序列的基础

### 在AI中的重要应用

- **贝叶斯分类器**：利用贝叶斯公式进行分类
- **朴素贝叶斯**：假设特征条件独立的分类算法
- **马尔可夫模型**：基于条件概率的序列建模
- **随机过程**：描述系统随时间的随机演化

这些概念为理解现代AI算法中的概率推理提供了重要的数学基础。
