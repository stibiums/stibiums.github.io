<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 人工智能中的编程 - 第6章: 矩阵乘法（Matrix Product） | STIBIUMS_WEB </title> <meta name="author" content="stibiums liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/touxiang.jpg?68b4199d95528c9129ff55a104244865"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stibiums.github.io/notes/AIP-ch06/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> STIBIUMS_WEB </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">人工智能中的编程 - 第6章: 矩阵乘法（Matrix Product）</h1> <p class="post-meta"> Created on September 24, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> notes   <i class="fa-solid fa-hashtag fa-sm"></i> AIP   ·   <i class="fa-solid fa-tag fa-sm"></i> AIP </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="矩阵乘法的重要性">矩阵乘法的重要性</h2> <p>矩阵乘法是数学和深度学习中的基础运算，在神经网络的各个组件中扮演核心角色：</p> <h3 id="深度学习中的应用">深度学习中的应用</h3> <p><strong>全连接层（Fully Connected Layers）</strong>：</p> <ul> <li>前向传播：$Y_{m \times n} = W_{m \times k} \times X_{k \times n}$</li> <li>反向传播：$\frac{\partial L}{\partial X_{k \times n}} = W_{m \times k}^T \times \frac{\partial L}{\partial y_{m \times n}}$</li> <li>权重梯度：$\frac{\partial L}{\partial W_{m \times k}} = \frac{\partial L}{\partial y_{m \times n}} \times X_{k \times n}^T$</li> </ul> <p><strong>基本矩阵乘法运算</strong>： \(c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} = \sum_{k=1}^n a_{ik}b_{kj}\)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/matrix_multiplication_basic-480.webp 480w,/assets/img/notes_img/AIP-ch06/matrix_multiplication_basic-800.webp 800w,/assets/img/notes_img/AIP-ch06/matrix_multiplication_basic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/matrix_multiplication_basic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="cpu上的矩阵乘法">CPU上的矩阵乘法</h2> <h3 id="通用矩阵乘法gemm">通用矩阵乘法（GEMM）</h3> <p>GEMM运算形式：$C = \alpha A \times B + \beta C$</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">sgemm_cpu</span><span class="p">(</span>
    <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">row</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">col</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span>
                <span class="n">alpha</span> <span class="o">*</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>复杂度分析</strong>：</p> <ul> <li>三重嵌套循环</li> <li>工作复杂度：$O(MNK)$</li> </ul> <h2 id="gpu上的矩阵乘法">GPU上的矩阵乘法</h2> <h3 id="朴素实现naive-implementation">朴素实现（Naive Implementation）</h3> <p>每个线程负责计算矩阵C中的一个元素：</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">sgemm_naive</span><span class="p">(</span>
    <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>执行配置</strong>：</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">dim3</span> <span class="nf">gridDim</span><span class="p">(</span><span class="n">Ceil</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">Ceil</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span> <span class="n">blockDim</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span>
<span class="n">sgemm_naive</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridDim</span><span class="p">,</span> <span class="n">blockDim</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/cuda_thread_layout-480.webp 480w,/assets/img/notes_img/AIP-ch06/cuda_thread_layout-800.webp 800w,/assets/img/notes_img/AIP-ch06/cuda_thread_layout-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/cuda_thread_layout.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="块量化问题tile-quantization">块量化问题（Tile Quantization）</h3> <p>朴素实现存在的问题：</p> <ul> <li>当矩阵大小不是块大小的整数倍时，边界块中的大量线程处于空闲状态</li> <li>造成计算资源浪费</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/tile_quantization_problem-480.webp 480w,/assets/img/notes_img/AIP-ch06/tile_quantization_problem-800.webp 800w,/assets/img/notes_img/AIP-ch06/tile_quantization_problem-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/tile_quantization_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="内存访问优化">内存访问优化</h2> <h3 id="线程布局优化">线程布局优化</h3> <p><strong>问题分析</strong>：</p> <ul> <li>不同线程访问相同列数据时产生非连续内存访问</li> <li>需要调整线程布局以实现合并访问</li> </ul> <h3 id="合并访问实现coalescing-access">合并访问实现（Coalescing Access）</h3> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">sgemm_coalesce</span><span class="p">(</span>
    <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="mi">32</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">32</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">col</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">sum</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>配置调整</strong>：</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">dim3</span> <span class="nf">gridDim</span><span class="p">(</span><span class="n">Ceil</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">Ceil</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span> <span class="n">blockDim</span><span class="p">(</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">32</span><span class="p">);</span>
</code></pre></div></div> <p><strong>性能提升</strong>：比朴素实现快约8倍</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/memory_coalescing_pattern-480.webp 480w,/assets/img/notes_img/AIP-ch06/memory_coalescing_pattern-800.webp 800w,/assets/img/notes_img/AIP-ch06/memory_coalescing_pattern-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/memory_coalescing_pattern.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="roofline性能模型">Roofline性能模型</h2> <h3 id="性能瓶颈分析">性能瓶颈分析</h3> <p><strong>带宽限制（Bandwidth Bound）</strong>：</p> <ul> <li>受内存系统数据传输速度限制</li> </ul> <p><strong>计算限制（Compute Bound）</strong>：</p> <ul> <li>受计算能力限制，当算术强度高于机器平衡点时</li> </ul> <p><strong>GPU程序特点</strong>：</p> <ul> <li>通常（但不总是）受带宽限制</li> <li>需要提高内存效率</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/roofline_model-480.webp 480w,/assets/img/notes_img/AIP-ch06/roofline_model-800.webp 800w,/assets/img/notes_img/AIP-ch06/roofline_model-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/roofline_model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="共享内存优化">共享内存优化</h2> <h3 id="块状矩阵乘法">块状矩阵乘法</h3> <p><strong>核心思想</strong>：</p> <ul> <li>将A和B的块从全局内存加载到共享内存</li> <li>每个线程仍负责C中的一个元素</li> <li>沿A的列和B的行移动数据块</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/shared_memory_tiling-480.webp 480w,/assets/img/notes_img/AIP-ch06/shared_memory_tiling-800.webp 800w,/assets/img/notes_img/AIP-ch06/shared_memory_tiling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/shared_memory_tiling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="共享内存实现">共享内存实现</h3> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">sgemm_shared_memory</span><span class="p">(</span>
    <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span> <span class="kt">float</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="n">beta</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">)</span> <span class="p">{</span>

    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">As</span><span class="p">[</span><span class="n">BLOCKSIZE</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span><span class="p">];</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">Bs</span><span class="p">[</span><span class="n">BLOCKSIZE</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span><span class="p">];</span>

    <span class="k">const</span> <span class="kt">int</span> <span class="n">cRow</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">cCol</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">uint</span> <span class="n">threadCol</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="n">BLOCKSIZE</span><span class="p">;</span>
    <span class="k">const</span> <span class="n">uint</span> <span class="n">threadRow</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="n">BLOCKSIZE</span><span class="p">;</span>

    <span class="c1">// 调整指针到起始位置</span>
    <span class="n">A</span> <span class="o">+=</span> <span class="n">cRow</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
    <span class="n">B</span> <span class="o">+=</span> <span class="n">cCol</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span><span class="p">;</span>
    <span class="n">C</span> <span class="o">+=</span> <span class="n">cRow</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">cCol</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span><span class="p">;</span>

    <span class="kt">float</span> <span class="n">tmp</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">bkIdx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">bkIdx</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">bkIdx</span> <span class="o">+=</span> <span class="n">BLOCKSIZE</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// 加载数据到共享内存</span>
        <span class="n">As</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">];</span>
        <span class="n">Bs</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">];</span>
        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="n">A</span> <span class="o">+=</span> <span class="n">BLOCKSIZE</span><span class="p">;</span>
        <span class="n">B</span> <span class="o">+=</span> <span class="n">BLOCKSIZE</span> <span class="o">*</span> <span class="n">N</span><span class="p">;</span>

        <span class="c1">// 执行点积运算</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">dotIdx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">dotIdx</span> <span class="o">&lt;</span> <span class="n">BLOCKSIZE</span><span class="p">;</span> <span class="o">++</span><span class="n">dotIdx</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">tmp</span> <span class="o">+=</span> <span class="n">As</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">+</span> <span class="n">dotIdx</span><span class="p">]</span> <span class="o">*</span>
                   <span class="n">Bs</span><span class="p">[</span><span class="n">dotIdx</span> <span class="o">*</span> <span class="n">BLOCKSIZE</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="n">C</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">]</span> <span class="o">=</span>
        <span class="n">alpha</span> <span class="o">*</span> <span class="n">tmp</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">C</span><span class="p">[</span><span class="n">threadRow</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">threadCol</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="性能对比">性能对比</h3> <table> <thead> <tr> <th>实现方式</th> <th>GFLOP/s</th> <th>相对cuBLAS性能</th> </tr> </thead> <tbody> <tr> <td>朴素实现</td> <td>309.0</td> <td>1.3%</td> </tr> <tr> <td>合并访问</td> <td>1986.5</td> <td>8.5%</td> </tr> <tr> <td>共享内存</td> <td>2980.3</td> <td>12.8%</td> </tr> </tbody> </table> <p><strong>内存限制</strong>：</p> <ul> <li>共享内存资源有限</li> <li>一个SM通常有100KB共享内存</li> <li>示例中使用了8KB共享内存（1024 × 2 × 4字节）</li> </ul> <h2 id="稀疏矩阵">稀疏矩阵</h2> <h3 id="稀疏矩阵的应用">稀疏矩阵的应用</h3> <p>稀疏矩阵广泛应用于：</p> <ul> <li>三角网格处理</li> <li>图神经网络</li> <li>科学计算</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/sparse_matrix_examples-480.webp 480w,/assets/img/notes_img/AIP-ch06/sparse_matrix_examples-800.webp 800w,/assets/img/notes_img/AIP-ch06/sparse_matrix_examples-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/sparse_matrix_examples.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="存储格式">存储格式</h3> <p><strong>坐标列表格式（COO）</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>矩阵: [5 0 0 0]    V = [5 8 3 6]
     [0 8 0 0]    COL_INDEX = [0 1 2 1]
     [0 0 3 0]    ROW_INDEX = [0 1 2 3]
     [0 6 0 0]
</code></pre></div></div> <p><strong>压缩稀疏行格式（CSR）</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>矩阵: [10 20  0  0  0  0]    V = [10 20 30 40 50 60 70 80]
     [ 0 30  0 40  0  0]    COL_INDEX = [0 1 1 3 2 3 4 5]
     [ 0  0 50 60 70  0]    ROW_INDEX = [0 2 4 7 8]
     [ 0  0  0  0  0 80]
</code></pre></div></div> <h3 id="pytorch稀疏张量">PyTorch稀疏张量</h3> <p><strong>COO格式示例</strong>：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">i</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sparse_coo_tensor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span><span class="p">.</span><span class="nf">to_dense</span><span class="p">()</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</code></pre></div></div> <p><strong>CSR格式示例</strong>：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">crow_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">col_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">csr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sparse_csr_tensor</span><span class="p">(</span><span class="n">crow_indices</span><span class="p">,</span> <span class="n">col_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span>
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">csr</span><span class="p">.</span><span class="nf">to_dense</span><span class="p">()</span>
<span class="nf">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
</code></pre></div></div> <h3 id="稀疏矩阵-向量乘法">稀疏矩阵-向量乘法</h3> <p><strong>算法步骤</strong>：</p> <ol> <li> <strong>映射操作</strong>：计算标量积 Value × Column × X</li> <li> <strong>分段扫描</strong>：使用cRow进行分段求和</li> </ol> <p><strong>示例</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>稀疏矩阵: [1 0 3]    向量: [x]    结果: [x + 3z]
         [2 1 0]           [y]          [2x + y]
         [0 4 3]           [z]          [4y + 3z]

Value: [1, 3, 2, 1, 4, 3]
Column: [0, 2, 0, 1, 1, 2]
cRow: [0, 2, 5, 6]
</code></pre></div></div> <p><strong>优化策略</strong>：将矩阵分解为Reduce和分段扫描操作</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/sparse_matrix_vector_speedup-480.webp 480w,/assets/img/notes_img/AIP-ch06/sparse_matrix_vector_speedup-800.webp 800w,/assets/img/notes_img/AIP-ch06/sparse_matrix_vector_speedup-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/sparse_matrix_vector_speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="cuda线性代数库">CUDA线性代数库</h2> <h3 id="cuda库家族">CUDA库家族</h3> <p><strong>CUDA编程语言</strong>：</p> <ul> <li>Thrust：基于STL的C++模板库</li> </ul> <p><strong>CUDA深度学习库</strong>：</p> <ul> <li>cuDNN：大多数开源深度学习框架的GPU组件</li> <li>TensorRT：高性能深度学习推理优化器和运行时</li> </ul> <p><strong>CUDA线性代数和数学库</strong>：</p> <ul> <li>cuBLAS：GPU加速的BLAS库，是GPU矩阵运算的最高性能实现</li> <li>cuSPARSE：处理稀疏矩阵</li> <li>cuRAND：GPU加速的随机数生成器</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/cuda_libraries_overview-480.webp 480w,/assets/img/notes_img/AIP-ch06/cuda_libraries_overview-800.webp 800w,/assets/img/notes_img/AIP-ch06/cuda_libraries_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/cuda_libraries_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="thrust库">Thrust库</h3> <p><strong>特性</strong>：</p> <ul> <li>基于标准模板库（STL）的C++模板库</li> <li>提供丰富的数据并行原语集合：scan、sort、reduce</li> <li>可组合实现复杂算法，代码简洁易读</li> </ul> <p><strong>向量操作示例</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;thrust/device_vector.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;thrust/host_vector.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">host_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">H</span><span class="p">(</span><span class="mi">4</span><span class="p">);</span>
    <span class="n">H</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">14</span><span class="p">;</span> <span class="n">H</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">20</span><span class="p">;</span> <span class="n">H</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">38</span><span class="p">;</span>

    <span class="c1">// 复制到设备</span>
    <span class="n">thrust</span><span class="o">::</span><span class="n">device_vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">D</span> <span class="o">=</span> <span class="n">H</span><span class="p">;</span>

    <span class="c1">// 修改元素</span>
    <span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">99</span><span class="p">;</span> <span class="n">D</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">88</span><span class="p">;</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Map操作实现</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;thrust/transform.h&gt;</span><span class="cp">
</span>
<span class="c1">// 计算 Y = -X</span>
<span class="n">thrust</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">X</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">Y</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span>
                 <span class="n">thrust</span><span class="o">::</span><span class="n">negate</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">());</span>

<span class="c1">// 自定义操作类</span>
<span class="k">class</span> <span class="nc">Saxpy</span> <span class="p">{</span>
<span class="nl">public:</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">a</span><span class="p">;</span>
    <span class="n">Saxpy</span><span class="p">(</span><span class="kt">float</span> <span class="n">_a</span><span class="p">)</span> <span class="o">:</span> <span class="n">a</span><span class="p">(</span><span class="n">_a</span><span class="p">)</span> <span class="p">{}</span>
    <span class="n">__host__</span> <span class="n">__device__</span> <span class="kt">float</span> <span class="k">operator</span><span class="p">()(</span><span class="k">const</span> <span class="kt">float</span><span class="o">&amp;</span> <span class="n">x</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">&amp;</span> <span class="n">y</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">};</span>

<span class="c1">// Y &lt;- A * X + Y</span>
<span class="n">thrust</span><span class="o">::</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">X</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">Y</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Y</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">Saxpy</span><span class="p">(</span><span class="n">A</span><span class="p">));</span>
</code></pre></div></div> <p><strong>其他操作</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 归约操作</span>
<span class="cp">#include</span> <span class="cpf">&lt;thrust/reduce.h&gt;</span><span class="cp">
</span><span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">thrust</span><span class="o">::</span><span class="n">reduce</span><span class="p">(</span><span class="n">D</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">D</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="mi">0</span><span class="p">,</span> <span class="n">thrust</span><span class="o">::</span><span class="n">plus</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">());</span>

<span class="c1">// 扫描操作</span>
<span class="cp">#include</span> <span class="cpf">&lt;thrust/scan.h&gt;</span><span class="cp">
</span><span class="n">thrust</span><span class="o">::</span><span class="n">inclusive_scan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span> <span class="o">+</span> <span class="mi">6</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>
<span class="n">thrust</span><span class="o">::</span><span class="n">exclusive_scan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span> <span class="o">+</span> <span class="mi">6</span><span class="p">,</span> <span class="n">data</span><span class="p">);</span>

<span class="c1">// 排序操作</span>
<span class="cp">#include</span> <span class="cpf">&lt;thrust/sort.h&gt;</span><span class="cp">
</span><span class="n">thrust</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span> <span class="o">+</span> <span class="n">N</span><span class="p">);</span>
</code></pre></div></div> <h2 id="blas基础线性代数子程序">BLAS（基础线性代数子程序）</h2> <h3 id="blas层次结构">BLAS层次结构</h3> <p><strong>Level 1</strong>：向量操作，线性时间复杂度</p> <ul> <li>点积、向量范数</li> <li>AXPY运算：$y \leftarrow \alpha x + y$</li> </ul> <p><strong>Level 2</strong>：矩阵-向量操作，二次时间复杂度</p> <ul> <li>通用矩阵-向量乘法（GEMV）：$y \leftarrow \alpha Ax + \beta y$</li> </ul> <p><strong>Level 3</strong>：矩阵-矩阵操作，三次时间复杂度</p> <ul> <li>通用矩阵乘法（GEMM）：$C \leftarrow \alpha AB + \beta C$</li> </ul> <h3 id="cublas使用示例">cuBLAS使用示例</h3> <p><strong>基本使用模式</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 步骤1：创建cuBLAS句柄</span>
<span class="n">cublasHandle_t</span> <span class="n">handle</span><span class="p">;</span>
<span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>

<span class="c1">// 步骤2：调用SGEMM</span>
<span class="n">cublasSgemm</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="p">...</span><span class="o">&lt;</span><span class="n">options</span><span class="o">&gt;</span><span class="p">..);</span>

<span class="c1">// 步骤3：销毁句柄</span>
<span class="n">cublasDestroy</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
</code></pre></div></div> <p><strong>GEMM实现示例</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cublas_v2.h&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">gemm_gpu</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">B</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">C</span><span class="p">,</span>
              <span class="k">const</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">k</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">lda</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">ldb</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">ldc</span> <span class="o">=</span> <span class="n">m</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">alf</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bet</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">alpha</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">alf</span><span class="p">,</span> <span class="o">*</span><span class="n">beta</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">bet</span><span class="p">;</span>

    <span class="n">cublasHandle_t</span> <span class="n">handle</span><span class="p">;</span>
    <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>

    <span class="n">cublasSgemm</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">CUBLAS_OP_N</span><span class="p">,</span> <span class="n">CUBLAS_OP_N</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
                <span class="n">A</span><span class="p">,</span> <span class="n">lda</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">ldb</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">ldc</span><span class="p">);</span>

    <span class="n">cublasDestroy</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="curand使用示例">cuRAND使用示例</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;curand.h&gt;</span><span class="cp">
</span>
<span class="kt">void</span> <span class="nf">matrix_init</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">A</span><span class="p">,</span> <span class="kt">int</span> <span class="n">rows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">cols</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">curandGenerator_t</span> <span class="n">prng</span><span class="p">;</span>
    <span class="n">curandCreateGenerator</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prng</span><span class="p">,</span> <span class="n">CURAND_RNG_PSEUDO_DEFAULT</span><span class="p">);</span>

    <span class="n">curandSetPseudoRandomGeneratorSeed</span><span class="p">(</span><span class="n">prng</span><span class="p">,</span>
                                      <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="n">clock</span><span class="p">());</span>

    <span class="n">curandGenerateUniform</span><span class="p">(</span><span class="n">prng</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span><span class="p">);</span>
    <span class="n">curandDestroyGenerator</span><span class="p">(</span><span class="n">prng</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="全连接层实现">全连接层实现</h2> <h3 id="pytorch中的全连接层">PyTorch中的全连接层</h3> <p><strong>张量形状变换</strong>：</p> <ul> <li>图像特征张量形状：[N, H, W, C] 或 [N, C, H, W]</li> <li>使用<code class="language-plaintext highlighter-rouge">torch.view</code>、<code class="language-plaintext highlighter-rouge">torch.reshape</code>、<code class="language-plaintext highlighter-rouge">torch.flatten</code>变换为[N, -1]</li> </ul> <p><strong>全连接层使用</strong>：</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 使用torch.nn.Linear（而非torch.nn.functional.linear）
# torch.nn.Linear帮助优化权重
</span><span class="n">layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="cuda实现">CUDA实现</h3> <p><strong>前向传播</strong>：</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">forward_fc</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">bias</span><span class="p">,</span>
                <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">in_features</span><span class="p">,</span> <span class="kt">int</span> <span class="n">out_features</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 矩阵乘法</span>
    <span class="n">gemm_gpu</span><span class="p">(</span><span class="n">CublasNoTrans</span><span class="p">,</span> <span class="n">CublasTrans</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span>
             <span class="mf">1.0</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>

    <span class="c1">// 添加偏置</span>
    <span class="n">gemm_gpu</span><span class="p">(</span><span class="n">CublasNoTrans</span><span class="p">,</span> <span class="n">CublasNoTrans</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
             <span class="mf">1.0</span><span class="p">,</span> <span class="n">ones_</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="全连接层的优缺点">全连接层的优缺点</h3> <p><strong>优点</strong>：</p> <ul> <li>表达能力强</li> <li>可以用GEMM轻松实现</li> </ul> <p><strong>缺点</strong>：</p> <ul> <li>需要大量参数（例如：200×200→1000的FC层需要200M参数）</li> <li>缺乏平移不变性</li> </ul> <h2 id="深度学习中的矩阵运算">深度学习中的矩阵运算</h2> <p>以AlexNet为例，神经网络包含多种运算类型：</p> <ul> <li>卷积层（Convolution）</li> <li>最大池化（Max Pooling）</li> <li>全连接层（Fully Connected Layer）</li> <li>Softmax和损失函数</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch06/alexnet_architecture-480.webp 480w,/assets/img/notes_img/AIP-ch06/alexnet_architecture-800.webp 800w,/assets/img/notes_img/AIP-ch06/alexnet_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch06/alexnet_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>全连接层在神经网络的最后阶段起到分类器的作用，通过矩阵乘法将特征映射到输出类别。</p> <h2 id="总结">总结</h2> <h3 id="矩阵乘法优化要点">矩阵乘法优化要点</h3> <ol> <li> <strong>内存访问优化</strong>：合并访问比分散访问更重要</li> <li> <strong>共享内存利用</strong>：通过数据局部性减少全局内存访问</li> <li> <strong>线程布局设计</strong>：避免warp内的分歧和空闲线程</li> <li> <strong>库函数使用</strong>：cuBLAS提供高度优化的实现</li> </ol> <h3 id="cuda生态系统价值">CUDA生态系统价值</h3> <p>CUDA线性代数和数学库为高性能GPU应用程序提供了重要基础：</p> <ul> <li> <strong>cuBLAS</strong>：矩阵运算的黄金标准</li> <li> <strong>cuSPARSE</strong>：稀疏矩阵运算支持</li> <li> <strong>Thrust</strong>：简化并行算法开发</li> <li> <strong>深度学习库</strong>：为AI应用提供专门优化</li> </ul> <p>这些工具和技术在深度学习、科学计算等领域发挥着关键作用，使得复杂的数值计算能够在GPU上高效执行。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ML/">notes of ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofvci/">notes of VCI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofaip/">notes of AIP</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_aimath/">notes of AI Math Fundamentals</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ICS/">notes of ICS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 stibiums liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?58fdb075e5aa03fbb8617845abde746c"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>