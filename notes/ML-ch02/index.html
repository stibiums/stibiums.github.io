<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ML - 2: Logistic Regression (逻辑回归) | STIBIUMS_WEB </title> <meta name="author" content="stibiums liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/touxiang.jpg?68b4199d95528c9129ff55a104244865"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stibiums.github.io/notes/ML-ch02/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> STIBIUMS_WEB </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ML - 2: Logistic Regression (逻辑回归)</h1> <p class="post-meta"> Created on September 18, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> notes   <i class="fa-solid fa-hashtag fa-sm"></i> ML   ·   <i class="fa-solid fa-tag fa-sm"></i> ML </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-二分类问题设定">1. 二分类问题设定</h2> <h3 id="问题定义">问题定义</h3> <ul> <li> <strong>输入</strong>: $X \in \mathbb{R}^d$</li> <li> <strong>输出</strong>: $y \in {0, 1}$ （负类/正类）</li> <li> <strong>使用线性模型</strong>: $f(x) = w^T x + b \in \mathbb{R}$</li> </ul> <h3 id="核心问题">核心问题</h3> <ol> <li>$f(x) \in \mathbb{R}$，但 $y \in {0, 1}$ —— 值域不匹配</li> <li> <strong>需要软预测</strong> (soft prediction)：预测概率 $P(y=1 \mid x)$ —— How likely?</li> </ol> <h3 id="解决方案">解决方案</h3> <p>我们需要使用 $f(x) = w^T x + b$ 来拟合 $P(y=1 \mid x=x_i)$</p> <p><strong>使用 Sigmoid 函数</strong>: $\sigma(z) = \frac{1}{1 + e^{-z}}$，将 $\mathbb{R} \rightarrow [0, 1]$</p> <h2 id="2-sigmoid函数">2. Sigmoid函数</h2> <h3 id="定义与性质">定义与性质</h3> \[\sigma(z) = \frac{1}{1 + e^{-z}}\] <ul> <li> <strong>定义域</strong>: $\mathbb{R}$，<strong>值域</strong>: $[0, 1]$</li> <li><strong>单调递增函数</strong></li> <li> <strong>对称性</strong>: $1 - \sigma(z) = \sigma(-z)$</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/ML-ch02/sigmoid-and-boundary-480.webp 480w,/assets/img/notes_img/ML-ch02/sigmoid-and-boundary-800.webp 800w,/assets/img/notes_img/ML-ch02/sigmoid-and-boundary-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/ML-ch02/sigmoid-and-boundary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sigmoid函数与线性决策边界" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="决策边界">决策边界</h3> <p>设置阈值 $\sigma(z) = 0.5$ 来输出硬预测：</p> <ul> <li>当 $w^T x + b &gt; 0$ 时，预测为正类</li> <li>当 $w^T x + b &lt; 0$ 时，预测为负类</li> <li> <strong>分离超平面</strong>: $w^T x + b = 0$</li> </ul> <h2 id="3-最大似然估计-mle">3. 最大似然估计 (MLE)</h2> <h3 id="如何找到参数-w-和-b">如何找到参数 $w$ 和 $b$？</h3> <p>使用<strong>最大似然估计</strong>，找到参数 $w, b$ 使得在整个训练数据上的似然 $P(y=y_i \mid x=x_i)$ 最大化。</p> <h3 id="似然函数">似然函数</h3> <p>对于单个样本，条件概率为：</p> \[P(y_i \mid x_i) = \begin{cases} \sigma(w^T x_i + b) &amp; \text{if } y_i = 1 \\ 1 - \sigma(w^T x_i + b) &amp; \text{if } y_i = 0 \end{cases}\] <p><strong>统一表示</strong>: \(P(y_i \mid x_i) = \sigma(w^T x_i + b)^{y_i} [1 - \sigma(w^T x_i + b)]^{1-y_i}\)</p> <p><strong>整体似然</strong>: \(\mathcal{L} = \prod_{i \in [n]} P(y_i \mid x_i; w, b)\)</p> <h3 id="最大化对数似然">最大化对数似然</h3> \[\log \mathcal{L} = \sum_{i \in [n]} [y_i \log(\sigma(w^T x_i + b)) + (1-y_i) \log(1-\sigma(w^T x_i + b))]\] <p><strong>等价于最小化</strong>: \(\min_{w,b} \sum_{i \in [n]} [-y_i \log(\sigma(w^T x_i + b)) - (1-y_i) \log(1-\sigma(w^T x_i + b))]\)</p> <p>这就是<strong>交叉熵损失函数</strong> (Cross Entropy Loss)。</p> <h2 id="4-交叉熵损失函数">4. 交叉熵损失函数</h2> <h3 id="信息论基础">信息论基础</h3> <h4 id="熵-entropy">熵 (Entropy)</h4> <p>对于概率分布 $P(y)$： \(H(P) = \sum_y P(y) \log \frac{1}{P(y)} = -\sum_y P(y) \log P(y)\)</p> <p>熵衡量了系统的不确定性。</p> <h4 id="交叉熵-cross-entropy">交叉熵 (Cross Entropy)</h4> <p>交叉熵涉及两个分布 $P$ 与 $Q$。其中 $P$ 为实际分布，$Q$ 为模型预测的分布。其度量的是用分布 $Q$ 编码分布 $P$ 所需的平均信息量：</p> \[H(P, Q) = -\sum_y P(y) \log Q(y)\] <h3 id="kl散度">KL散度</h3> \[KL(P||Q) = \sum_y P(y) \log \frac{P(y)}{Q(y)} \geq 0\] \[= -\sum_y P(y) \log Q(y) - (-\sum_y P(y) \log P(y)) = H(P, Q) - H(P)\] <h2 id="5-寻找闭式解">5. 寻找闭式解</h2> <h3 id="参数重写">参数重写</h3> <p>定义增广矩阵形式：</p> <ul> <li>$\hat{X} = \begin{bmatrix} X \ \mathbf{1}^T \end{bmatrix} \in \mathbb{R}^{(d+1) \times n}$</li> <li>$\hat{w} = \begin{bmatrix} w \ b \end{bmatrix} \in \mathbb{R}^{d+1}$</li> </ul> <p>则 $f(x) = w^T x + b = \hat{w}^T \hat{x}$</p> <h3 id="损失函数">损失函数</h3> \[L(\hat{w}) = -\sum_{i \in [n]} [y_i \log \sigma(\hat{w}^T \hat{x}_i) + (1-y_i) \log(1-\sigma(\hat{w}^T \hat{x}_i))]\] \[= -\sum_{i \in [n]} [y_i \hat{w}^T \hat{x}_i - \log(1 + e^{\hat{w}^T \hat{x}_i})]\] <h3 id="梯度计算">梯度计算</h3> \[\frac{\partial L(\hat{w})}{\partial \hat{w}} = -\sum_{i \in [n]} \left[y_i - \frac{e^{\hat{w}^T \hat{x}_i}}{1 + e^{\hat{w}^T \hat{x}_i}}\right] \hat{x}_i\] \[= -\sum_{i \in [n]} [y_i - \sigma(\hat{w}^T \hat{x}_i)] \hat{x}_i\] \[= -\sum_{i \in [n]} [y_i - P(y=1 \mid x_i)] \hat{x}_i\] <h3 id="梯度下降">梯度下降</h3> \[\hat{w} \leftarrow \hat{w} + \alpha \sum_{i \in [n]} [y_i - P(y=1 \mid x_i)] \hat{x}_i\] <h3 id="收敛条件">收敛条件</h3> <p>当 $\frac{\partial L(\hat{w})}{\partial \hat{w}} = 0$ 时，即存在 $P(y=1 \mid x_i) = y_i$，称为<strong>线性可分</strong>。</p> <h2 id="6-线性可分情况的讨论">6. 线性可分情况的讨论</h2> <p>在线性可分情况下，存在非常多的分割超平面：</p> <ol> <li><strong>大部分问题并非线性可分</strong></li> <li> <strong>线性可分时</strong>，加入 L2 正则化便可找到唯一解： \(\min_{\hat{w}} L(\hat{w}) + \frac{\lambda}{2} ||\hat{w}||^2\)</li> </ol> <h2 id="7-为什么不能使用平方损失函数">7. 为什么不能使用平方损失函数？</h2> <ol> <li><strong>$y_i \in {0, 1}$ 无数值意义</strong></li> <li><strong>受离群值影响严重</strong></li> <li><strong>缺乏概率解释</strong></li> </ol> <h2 id="8-多分类情况softmax回归">8. 多分类情况：Softmax回归</h2> <h3 id="问题设定">问题设定</h3> <p>$y \in {1, 2, …, K}$ —— $K$ 分类问题</p> <h3 id="k个分类器">K个分类器</h3> \[f_k(x) = w_k^T x + b_k, \quad k \in [K]\] <h3 id="softmax函数">Softmax函数</h3> \[P(y=k \mid x) = \frac{\exp(w_k^T x + b_k)}{\sum_{j=1}^K \exp(w_j^T x + b_j)} \in [0, 1]\] <h3 id="性质">性质</h3> <ul> <li>$\sum_{j \in [K]} P(y=j \mid x) = 1$ （归一化的概率）</li> <li>当 $f_k(x) » f_j(x)$ 时，$P(y=k \mid x) \approx 1$</li> </ul> <h3 id="mle目标">MLE目标</h3> \[\max \sum_{i \in [n]} \log \frac{\exp(w_{y_i}^T x_i + b_{y_i})}{\sum_{j \in [K]} \exp(w_j^T x_i + b_j)}\] <p><strong>注意</strong>: 当 $K=2$ 时，softmax回归等价于逻辑回归。</p> <h2 id="9-对于线性回归可否由mle导出损失">9. 对于线性回归，可否由MLE导出损失？</h2> <h3 id="高斯分布">高斯分布</h3> <p>\(x \sim \mathcal{N}(\mu, \sigma^2)\)，其中 $\mu$ 为均值，$\sigma^2$ 为方差</p> \[P(x) = \mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\] <p>在 $2\sigma$ 之内占 $95\%$</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/ML-ch02/gaussian-crossentropy-480.webp 480w,/assets/img/notes_img/ML-ch02/gaussian-crossentropy-800.webp 800w,/assets/img/notes_img/ML-ch02/gaussian-crossentropy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/ML-ch02/gaussian-crossentropy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="高斯分布与交叉熵损失函数" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="噪声模型">噪声模型</h3> <p>\(y = w^T x + b + \epsilon\) 其中 $\epsilon$ 服从高斯分布：$\epsilon \sim \mathcal{N}(0, \sigma^2)$</p> <p>此时对 $y$ 进行概率建模： \(P(y \mid x; w, b, \sigma^2) = \mathcal{N}(w^T x + b, \sigma^2)\)</p> <h3 id="log-likelihood">Log-likelihood</h3> \[\max \sum_{i \in [n]} \log \mathcal{N}(y_i|w^T x_i + b, \sigma^2)\] \[= \max \sum_{i \in [n]} \left[\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{1}{2\sigma^2}(y_i - (w^T x_i + b))^2\right]\] \[= \min \frac{1}{2\sigma^2} \sum_{i \in [n]} (y_i - (w^T x_i + b))^2\] <p><strong>结论</strong>：推出了平方损失的等价形式！</p> <h2 id="10-最大后验估计-map">10. 最大后验估计 (MAP)</h2> <h3 id="先验分布">先验分布</h3> \[P(\hat{w}) = \mathcal{N}(\hat{w}|0, \sigma_w^2 I)\] <p>$\hat{w}$ 也是随机变量</p> <h3 id="后验分布">后验分布</h3> \[P(\hat{w} \mid y, X) = \frac{P(y \mid X, \hat{w}) P(\hat{w})}{P(y \mid X)} \propto P(y \mid X, \hat{w}) P(\hat{w})\] <h3 id="map目标">MAP目标</h3> \[\max P(\hat{w} \mid y, X) = \max P(y \mid X, \hat{w}) P(\hat{w})\] \[\propto \max \left[\exp\left(-\frac{\sum_{i \in [n]}(y_i - \hat{w}^T \hat{x}_i)^2}{2\sigma^2}\right) \exp\left(-\frac{\hat{w}^T\hat{w}}{2\sigma_w^2}\right)\right]\] <p>取负log，去掉无关项： \(\min \sum_{i \in [n]} (y_i - \hat{w}^T \hat{x}_i)^2 + \frac{\sigma^2}{\sigma_w^2} ||\hat{w}||^2\)</p> <p><strong>结论</strong>：<strong>岭回归</strong>！！</p> <h2 id="总结">总结</h2> <p>逻辑回归将二分类问题转化为概率建模问题：</p> <ol> <li>使用 Sigmoid 函数将线性输出映射到概率空间</li> <li>通过最大似然估计学习参数，等价于最小化交叉熵损失</li> <li>梯度下降求解，线性可分时需要正则化</li> <li>自然扩展到多分类（Softmax）</li> <li>从概率视角统一了线性回归（高斯分布）和逻辑回归（伯努利分布）</li> <li>MAP 估计引出正则化的必要性</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ML/">notes of ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofvci/">notes of VCI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofaip/">notes of AIP</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_aimath/">notes of AI Math Fundamentals</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ICS/">notes of ICS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 stibiums liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 12, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?58fdb075e5aa03fbb8617845abde746c"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>