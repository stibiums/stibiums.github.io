<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 人工智能中的编程 - 第4章: 并行算法（Parallel Algorithms） | STIBIUMS_WEB </title> <meta name="author" content="stibiums liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/touxiang.jpg?68b4199d95528c9129ff55a104244865"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stibiums.github.io/notes/AIP-ch04/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> STIBIUMS_WEB </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">人工智能中的编程 - 第4章: 并行算法（Parallel Algorithms）</h1> <p class="post-meta"> Created on September 24, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> notes   <i class="fa-solid fa-hashtag fa-sm"></i> AIP   ·   <i class="fa-solid fa-tag fa-sm"></i> AIP </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="上节课回顾">上节课回顾</h2> <h3 id="gpu内存">GPU内存</h3> <ul> <li>内存管理；Tensor</li> <li>Local / Shared / Global内存层次结构</li> </ul> <h3 id="gpu硬件">GPU硬件</h3> <ul> <li>并行化线程和块</li> <li>SIMT：单指令多线程</li> </ul> <h3 id="同步">同步</h3> <ul> <li>屏障（Barrier）</li> <li>原子操作（Atomic operations）</li> </ul> <h3 id="通信模式">通信模式</h3> <ul> <li>Map, Gather, Scatter, Stencil, Transpose</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/parallel_patterns-480.webp 480w,/assets/img/notes_img/AIP-ch04/parallel_patterns-800.webp 800w,/assets/img/notes_img/AIP-ch04/parallel_patterns-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/parallel_patterns.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="主要内容">主要内容</h2> <p>从通信模式扩展到并行算法：</p> <table> <thead> <tr> <th>通信模式</th> <th>并行算法</th> </tr> </thead> <tbody> <tr> <td>Map, Gather, Scatter</td> <td>Reduce, Scan, Histogram</td> </tr> <tr> <td>Stencil, Transpose</td> <td>Sort, Segment Reduce</td> </tr> </tbody> </table> <h2 id="并行归约parallel-reduction">并行归约（Parallel Reduction）</h2> <h3 id="基本概念">基本概念</h3> <p>计算 $s = \sum_i x_i$ 的并行版本是最重要的并行算法模式之一。</p> <p><strong>核心思想</strong>：利用运算的结合律，将线性的串行计算转换为对数深度的并行计算。</p> <p><strong>应用场景</strong>：</p> <ul> <li> <strong>张量统计计算</strong>：<code class="language-plaintext highlighter-rouge">torch.mean(input)</code>, <code class="language-plaintext highlighter-rouge">torch.sum(input)</code>, <code class="language-plaintext highlighter-rouge">torch.max(input)</code> </li> <li> <strong>神经网络训练中的损失函数计算</strong>：交叉熵损失、均方误差等</li> <li> <strong>Batch Normalization</strong>： <ul> <li>计算批次均值：$\mu = \frac{1}{N}\sum_{i=1}^{N} x_i$</li> <li>计算批次方差：$\sigma^2 = \frac{1}{N}\sum_{i=1}^{N} (x_i - \mu)^2$</li> <li>标准化特征：$\hat{x}_i = \frac{x_i-\mu}{\sqrt{\sigma^2 + \epsilon}}$</li> </ul> </li> <li> <strong>Group Normalization</strong>：在通道组内进行类似的统计计算</li> <li> <strong>向量范数计算</strong>：$|x|<em>2 = \sqrt{\sum</em>{i=1}^{N} x_i^2}$</li> </ul> <h3 id="复杂度分析">复杂度分析</h3> <p><strong>CPU串行实现</strong>：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">h_in</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>工作复杂度：$O(N)$</li> <li>步复杂度：$O(N)$ - 串行，在GPU上运行缓慢</li> </ul> <p><strong>GPU并行实现</strong>：</p> <ul> <li>加法运算满足结合律</li> <li>使用”浅层树”结构进行并行加法</li> <li>工作复杂度：$O(N)$</li> <li>步复杂度：$O(\log N)$</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/reduction_tree-480.webp 480w,/assets/img/notes_img/AIP-ch04/reduction_tree-800.webp 800w,/assets/img/notes_img/AIP-ch04/reduction_tree-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/reduction_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="寻址策略详解">寻址策略详解</h3> <p>GPU并行归约中的寻址模式对性能至关重要，主要有两种策略：</p> <h4 id="1-交错寻址-interleaved-addressing">1. 交错寻址 (Interleaved Addressing)</h4> <p><strong>特点</strong>：</p> <ul> <li>线程访问的内存地址跳跃性很大</li> <li>线程间距随步骤增加：1, 2, 4, 8, …</li> <li>容易造成内存访问不合并</li> </ul> <p><strong>问题</strong>：</p> <ul> <li>内存访问模式不规律，影响缓存效率</li> <li>Warp内线程访问的地址分散，无法利用合并访问</li> </ul> <h4 id="2-顺序寻址-sequential-addressing">2. 顺序寻址 (Sequential Addressing)</h4> <p><strong>特点</strong>：</p> <ul> <li>活跃线程总是连续的：0, 1, 2, 3, …</li> <li>内存访问地址连续，利于合并访问</li> <li>更好的数据局部性</li> </ul> <p><strong>优势</strong>：</p> <ul> <li> <strong>更好的数据局部性</strong>：连续内存访问提高缓存命中率</li> <li> <strong>跨块无冲突访问</strong>：不同块之间的内存访问模式不会冲突</li> <li> <strong>合并内存访问</strong>：Warp内线程访问连续地址，提高带宽利用率</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/addressing_comparison-480.webp 480w,/assets/img/notes_img/AIP-ch04/addressing_comparison-800.webp 800w,/assets/img/notes_img/AIP-ch04/addressing_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/addressing_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="使用全局内存的并行归约">使用全局内存的并行归约</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reduce_global</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">d_out</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_in</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span> <span class="o">&amp;&amp;</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// 确保同一阶段的所有加法完成</span>
    <span class="p">}</span>

    <span class="c1">// 只有线程0将此块的结果写入全局内存</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span> <span class="n">d_out</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>问题</strong>：</p> <ul> <li>合并的全局内存访问？</li> <li>能否使用共享内存提升速度？</li> </ul> <h3 id="使用共享内存的并行归约">使用共享内存的并行归约</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reduce_shared</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">d_out</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_in</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="c1">// 在kernel调用时使用 &lt;&lt;&lt;b, t, shmem&gt;&gt;&gt; 动态分配</span>
    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">shared</span><span class="p">[];</span>
    <span class="n">shared</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">?</span> <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// 确保整个块已加载！</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">s</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">shared</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shared</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// 确保同一阶段的所有加法完成！</span>
    <span class="p">}</span>

    <span class="c1">// 只有线程0将此块的结果写入全局内存</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span> <span class="n">d_out</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">shared</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>优势</strong>：使用共享内存减少全局内存访问</p> <h3 id="两级归约实现">两级归约实现</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">reduce</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">d_out</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">d_in</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_tmp</span><span class="p">;</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_tmp</span><span class="p">,</span> <span class="n">CudaGetBlocks</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

    <span class="kt">int</span> <span class="n">num</span> <span class="o">=</span> <span class="n">N</span><span class="p">;</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">ptr_in</span> <span class="o">=</span> <span class="n">d_in</span><span class="p">;</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">ptr_out</span> <span class="o">=</span> <span class="n">d_tmp</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">kShared</span> <span class="o">=</span> <span class="n">kThreadsNum</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

    <span class="k">while</span> <span class="p">(</span><span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="n">CudaGetBlocks</span><span class="p">(</span><span class="n">num</span><span class="p">);</span>
        <span class="c1">// 动态分配共享内存 &lt;&lt;&lt;b, t, shmem&gt;&gt;&gt;</span>
        <span class="n">reduce_shared</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">kThreadsNum</span><span class="p">,</span> <span class="n">kShared</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">ptr_out</span><span class="p">,</span> <span class="n">ptr_in</span><span class="p">,</span> <span class="n">num</span><span class="p">);</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">;</span>
        <span class="n">std</span><span class="o">::</span><span class="n">swap</span><span class="p">(</span><span class="n">ptr_in</span><span class="p">,</span> <span class="n">ptr_out</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">ptr_in</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToDevice</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_tmp</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="直方图归约的应用">直方图：归约的应用</h3> <p><strong>CPU实现</strong>：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 计算bin索引</span>
    <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">h_in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">%</span> <span class="n">kBinCount</span><span class="p">;</span>
    <span class="n">h_bins</span><span class="p">[</span><span class="n">bin</span><span class="p">]</span><span class="o">++</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>朴素GPU实现</strong>：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">naive_histo</span><span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="n">d_bins</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">d_in</span><span class="p">,</span>
                           <span class="k">const</span> <span class="kt">int</span> <span class="n">kBinCount</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">d_in</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">%</span> <span class="n">kBinCount</span><span class="p">;</span>
    <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">d_bins</span><span class="p">[</span><span class="n">bin</span><span class="p">]),</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// "读-修改-写"操作</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>问题</strong>：<code class="language-plaintext highlighter-rouge">atomicAdd</code>虽然保证正确性，但速度较慢</p> <p><strong>优化策略 - 分层直方图</strong>：</p> <p>考虑一个具体的例子：</p> <ul> <li> <strong>数据规模</strong>：$512^2 = 262,144$ 个元素</li> <li> <strong>直方图桶数</strong>：10个桶</li> <li> <strong>GPU配置</strong>：512个块，每块512个线程</li> </ul> <p><strong>分层实现步骤</strong>：</p> <ol> <li> <strong>第一阶段</strong>：每个线程块维护局部直方图 <ul> <li>每个块处理512个元素</li> <li>使用共享内存存储局部直方图（10个桶）</li> <li>块内使用原子操作更新局部直方图</li> </ul> </li> <li> <strong>第二阶段</strong>：合并所有局部直方图 <ul> <li>512个局部直方图需要归约为1个全局直方图</li> <li>对每个桶使用并行归约算法</li> <li>最终得到完整的全局直方图</li> </ul> </li> </ol> <p><strong>性能优势</strong>：</p> <ul> <li>减少全局原子操作的冲突</li> <li>提高内存访问的局部性</li> <li>利用共享内存的高带宽</li> </ul> <h2 id="并行扫描parallel-scan">并行扫描（Parallel Scan）</h2> <h3 id="基本概念详解">基本概念详解</h3> <p><strong>并行扫描（Prefix Sum）</strong> 是另一个基础的并行算法模式。</p> <p><strong>定义</strong>：</p> <ul> <li> <strong>包含扫描</strong>：$y_i = \bigoplus_{j=0}^{i} x_j$，其中$\bigoplus$是结合运算符</li> <li> <strong>排除扫描</strong>：$y_i = \bigoplus_{j=0}^{i-1} x_j$</li> </ul> <p><strong>PyTorch中的<code class="language-plaintext highlighter-rouge">torch.cumsum</code></strong>：</p> <ul> <li> <strong>输入</strong>：(1, 2, 3, 4)</li> <li> <strong>包含扫描</strong>：(1, 3, 6, 10) — 每个位置包含到当前位置的累积</li> <li> <strong>排除扫描</strong>：(0, 1, 3, 6) — 每个位置不包含当前元素</li> </ul> <p><strong>算法重要性</strong>：</p> <ul> <li>在串行编程中看似简单，但在并行计算中是核心构建块</li> <li>可以解决许多传统上难以并行化的问题</li> <li>是许多复杂并行算法的基础</li> </ul> <p><strong>典型应用场景</strong>：</p> <ol> <li> <strong>并行紧缩 (Parallel Compaction)</strong>：</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>  <span class="c1"># 使用布尔索引进行紧缩
</span></code></pre></div></div> <ol> <li> <strong>并行排序算法</strong>：基数排序、快速排序的分区操作</li> <li> <strong>图算法</strong>：广度优先搜索、连通分量计算</li> <li> <strong>字符串算法</strong>：并行字符串匹配、后缀数组构建</li> <li> <strong>几何算法</strong>：凸包计算、最近点对问题</li> </ol> <h3 id="cpu实现">CPU实现</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>  <span class="c1">// 比归约多一行代码</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>工作复杂度：$O(N)$</li> <li>步复杂度：$O(N)$ - 串行，GPU上运行缓慢</li> </ul> <h3 id="gpu并行实现">GPU并行实现</h3> <p>给定并行归约算法，$s_k$是对$x_0$到$x_k$的并行归约：</p> <ul> <li>步复杂度：$O(\log N)$</li> <li>朴素实现的工作复杂度：$O(N^2)$</li> <li>优化后的工作复杂度：$O(N \log N)$</li> </ul> <h3 id="hillissteele包含扫描">Hillis/Steele包含扫描</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/hillis_steele_scan-480.webp 480w,/assets/img/notes_img/AIP-ch04/hillis_steele_scan-800.webp 800w,/assets/img/notes_img/AIP-ch04/hillis_steele_scan-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/hillis_steele_scan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>算法步骤</strong>：</p> <ul> <li> <strong>步骤1</strong>：添加直接邻居 n = 1</li> <li> <strong>步骤2</strong>：添加距离为2的邻居 n = 2</li> <li> <strong>步骤3</strong>：添加距离为4的邻居 n = 4</li> <li>一般地：n = $2^{step}$</li> </ul> <p><strong>复杂度</strong>：</p> <ul> <li>步数：$O(\log n)$</li> <li>工作量：$O(n \log n)$ （矩形区域的维度）</li> </ul> <h3 id="blelloch扫描工作高效算法">Blelloch扫描（工作高效算法）</h3> <p>更高效的$O(N)$工作量实现：</p> <p><strong>两阶段算法</strong>：</p> <ol> <li> <strong>上扫描阶段</strong>：类似归约操作</li> <li> <strong>下扫描阶段</strong>：分发部分和</li> </ol> <p><strong>复杂度</strong>：</p> <ul> <li>步复杂度：$O(2\log N)$</li> <li>工作复杂度：$O(N)$</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/blelloch_scan-480.webp 480w,/assets/img/notes_img/AIP-ch04/blelloch_scan-800.webp 800w,/assets/img/notes_img/AIP-ch04/blelloch_scan-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/blelloch_scan.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="并行紧缩">并行紧缩</h3> <p>使用扫描实现数组紧缩：</p> <ol> <li><strong>运行判定条件</strong></li> <li> <strong>创建数组</strong>：True = 1，False = 0</li> <li> <strong>运行排除扫描</strong>：输出为剩余输入的地址</li> <li><strong>将输入复制到输出数组</strong></li> </ol> <p><strong>示例</strong>：</p> <ul> <li>输入：[1, 2, 3, 4, 5, 6, 7, 8]</li> <li>判定：[T, F, T, F, T, F, T, F]</li> <li>扫描：[1, -, 3, -, 5, -, 7, -]</li> <li>输出：[1, 3, 5, 7]</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/parallel_compact-480.webp 480w,/assets/img/notes_img/AIP-ch04/parallel_compact-800.webp 800w,/assets/img/notes_img/AIP-ch04/parallel_compact-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/parallel_compact.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="分段扫描">分段扫描</h3> <p>对输入数组的任意连续分区执行扫描：</p> <p><strong>示例</strong>：</p> <ul> <li> <strong>输入</strong>：[[1, 2], [6, 7, 1], [1, 2, 3, 4]]</li> <li> <strong>排除扫描</strong>：[[0,1], [0, 6, 13], [0,1,3,6]]</li> </ul> <p><strong>头标志表示法</strong>：</p> <ul> <li>Flag = [1, 0, 1, 0, 0, 1, 0, 0, 0]</li> <li>Data = [1, 2, 6, 7, 1, 1, 2, 3, 4]</li> </ul> <p><strong>复杂度</strong>：分段扫描的步复杂度也是$O(\log N)$</p> <h2 id="矩阵转置transpose">矩阵转置（Transpose）</h2> <h3 id="cpu实现-1">CPU实现</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">transpose</span><span class="p">(</span><span class="kt">float</span> <span class="n">in</span><span class="p">[],</span> <span class="kt">float</span> <span class="n">out</span><span class="p">[])</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// out(j,i) = in(i,j)</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="gpu实现策略">GPU实现策略</h3> <p><strong>策略2：每行一个线程</strong></p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">transpose_per_row</span><span class="p">(</span><span class="kt">float</span> <span class="n">in</span><span class="p">[],</span> <span class="kt">float</span> <span class="n">out</span><span class="p">[])</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// out(j,i) = in(i,j)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>策略3：每元素一个线程（最大并行度）</strong></p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">transpose_per_element</span><span class="p">(</span><span class="kt">float</span> <span class="n">in</span><span class="p">[],</span> <span class="kt">float</span> <span class="n">out</span><span class="p">[])</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="c1">// out(j,i) = in(i,j)</span>
    <span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>问题</strong>：最大并行度并不总是最佳选择</p> <h3 id="内存访问优化">内存访问优化</h3> <p>大多数GPU程序是内存受限的：</p> <ul> <li>最后的实现：合并读取，分散写入</li> <li>目标：合并读取，合并写入</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/transpose_memory-480.webp 480w,/assets/img/notes_img/AIP-ch04/transpose_memory-800.webp 800w,/assets/img/notes_img/AIP-ch04/transpose_memory-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/transpose_memory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="分块转置实现">分块转置实现</h3> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">transpose_tiled</span><span class="p">(</span><span class="kt">float</span> <span class="n">in</span><span class="p">[],</span> <span class="kt">float</span> <span class="n">out</span><span class="p">[])</span> <span class="p">{</span>
    <span class="c1">// (i,j) 是瓦片角点</span>
    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

    <span class="c1">// 从全局内存合并读取</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tile</span><span class="p">[</span><span class="n">K</span><span class="p">][</span><span class="n">K</span><span class="p">];</span>
    <span class="n">tile</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">x</span><span class="p">)];</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="c1">// 向全局内存合并写入</span>
    <span class="n">out</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="n">x</span><span class="p">)]</span> <span class="o">=</span> <span class="n">tile</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// 启动kernel</span>
<span class="n">dim3</span> <span class="nf">blocks</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span> <span class="o">/</span> <span class="n">K</span><span class="p">);</span>
<span class="n">dim3</span> <span class="nf">threads</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">);</span>
<span class="n">transpose_tiled</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/AIP-ch04/tiled_transpose-480.webp 480w,/assets/img/notes_img/AIP-ch04/tiled_transpose-800.webp 800w,/assets/img/notes_img/AIP-ch04/tiled_transpose-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/AIP-ch04/tiled_transpose.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="算法总结">算法总结</h2> <h3 id="并行归约总结">并行归约总结</h3> <ul> <li>使用浅层树结构实现并行归约</li> <li>工作复杂度：$O(N)$；步复杂度：$O(\log N)$</li> <li>要求运算符为二元且满足结合律：SUM, MULTIPLY, MIN, MAX, AND, OR</li> <li>利用数据局部性和共享内存提升效率</li> </ul> <h3 id="并行扫描总结">并行扫描总结</h3> <ul> <li>也称为前缀和、累积和</li> <li>包含扫描和排除扫描</li> <li>工作复杂度：$O(N)$/$O(N \log N)$；步复杂度：$O(\log N)$</li> <li>要求运算符为二元且满足结合律：SUM, MULTIPLY, MIN, MAX, AND, OR</li> <li>可用于解决许多难以并行化的问题，如并行紧缩</li> </ul> <h3 id="关键原则与设计考虑">关键原则与设计考虑</h3> <p><strong>算法选择原则</strong>：</p> <ol> <li> <p><strong>运算符要求</strong>：所有并行归约和扫描算法都要求运算符为二元且满足结合律</p> <ul> <li>满足条件：+, ×, min, max, ∧, ∨</li> <li>不满足条件：- (减法), ÷ (除法)</li> </ul> </li> <li> <p><strong>复杂度权衡</strong>：</p> <ul> <li> <strong>工作复杂度</strong>：总计算量，影响算法效率</li> <li> <strong>步复杂度</strong>：并行深度，影响执行时间</li> <li>Hillis/Steele: 简单实现，但工作量大 $O(N \log N)$</li> <li>Blelloch: 复杂实现，但工作高效 $O(N)$</li> </ul> </li> <li> <p><strong>内存访问模式</strong>：</p> <ul> <li>合并访问比最大并行度更重要</li> <li>利用共享内存提高数据局部性</li> <li>避免分散的内存访问模式</li> </ul> </li> <li> <p><strong>实际应用考虑</strong>：</p> <ul> <li> <strong>并行直方图</strong>：图像处理、数据分析中的频率统计</li> <li> <strong>并行紧缩</strong>：数据过滤、稀疏矩阵操作</li> <li> <strong>矩阵转置</strong>：线性代数库、深度学习框架的核心操作</li> </ul> </li> </ol> <h2 id="编译和执行">编译和执行</h2> <p><strong>开发环境设置</strong>：</p> <ul> <li>安装Visual Studio (Windows) / gcc (Linux)</li> <li>安装CUDA Toolkit：https://developer.nvidia.com/cuda-downloads</li> <li>设置环境变量</li> <li>测试编译环境：<code class="language-plaintext highlighter-rouge">nvcc --version</code> </li> </ul> <p><strong>编译命令</strong>：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc <span class="nt">-o</span> relu relu.cu
</code></pre></div></div> <h2 id="总结与展望">总结与展望</h2> <h3 id="核心概念回顾">核心概念回顾</h3> <p>本讲介绍了GPU并行算法的核心模式：</p> <ol> <li> <strong>并行归约</strong>：从$O(N)$步复杂度的串行算法到$O(\log N)$步复杂度的并行算法</li> <li> <strong>并行扫描</strong>：看似简单的前缀和操作，实际上是解决复杂并行问题的万能工具</li> <li> <strong>内存优化</strong>：通过合理的内存访问模式和共享内存使用提升性能</li> </ol> <h3 id="在深度学习中的应用">在深度学习中的应用</h3> <p>这些并行算法构成了现代深度学习框架的基础：</p> <ul> <li> <strong>Batch Normalization</strong>：使用并行归约计算批次统计信息</li> <li> <strong>注意力机制</strong>：Softmax操作需要归约（求和）和扫描（前缀和）</li> <li> <strong>梯度聚合</strong>：分布式训练中的梯度归约操作</li> <li> <strong>动态图构建</strong>：使用并行紧缩过滤活跃的计算节点</li> <li> <strong>矩阵运算</strong>：GEMM操作中的数据重排需要高效的转置</li> </ul> <h3 id="性能优化原则">性能优化原则</h3> <p>通过本讲学习，我们理解了GPU编程的核心原则：</p> <ul> <li> <strong>算法设计</strong>：选择适合并行的算法模式</li> <li> <strong>内存管理</strong>：优化内存访问模式，减少带宽瓶颈</li> <li> <strong>同步开销</strong>：在正确性和性能之间找到平衡</li> </ul> <p>理解这些基础算法模式，对设计高效的GPU程序和深度学习系统至关重要。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ML/">notes of ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofvci/">notes of VCI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofaip/">notes of AIP</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_aimath/">notes of AI Math Fundamentals</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ICS/">notes of ICS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 stibiums liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 12, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?58fdb075e5aa03fbb8617845abde746c"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>