<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ML - 1: Linear Regression (线性回归) | STIBIUMS_WEB </title> <meta name="author" content="stibiums liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/touxiang.jpg?68b4199d95528c9129ff55a104244865"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stibiums.github.io/notes/ML-ch01/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> STIBIUMS_WEB </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">ML - 1: Linear Regression (线性回归)</h1> <p class="post-meta"> Created on September 10, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> notes   <i class="fa-solid fa-hashtag fa-sm"></i> ML   ·   <i class="fa-solid fa-tag fa-sm"></i> ML </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="线性回归-linear-regression">线性回归 (Linear Regression)</h2> <ul> <li>$D = {(x_i,y_i)}$ 为训练集，其中 $x_i \in \mathbb{R}^d,y\in \mathbb{R}$；</li> <li>线性模型：$f(x) = w^Tx + b$，其中 $w\in \mathbb{R}^d,b \in R$，分别称为权重（weight）和偏置（bias）。 $w$ 本质上是在对 $x$ 的每一维进行加权求和。</li> </ul> <p>我们根据什么来决定 $w,b$ 的值呢？我们使用到ERM（经验风险最小化）原则：使用损失函数来进行衡量，并使损失函数最小化。</p> <p>我们会使用平方损失函数（squared loss function）：</p> \[L(f(x_i),y_i) = (f(x_i)-y_i)^2 = (w^Tx_i + b - y_i)^2\] <p>于是经验风险（empirical risk）为：</p> \[L(f) = \frac{1}{n}\sum_{i=1}^n (w^Tx_i + b - y_i)^2\] <p>我们要做的就是最小化经验风险：</p> \[\min_{w,b} L(f) = \min_{w,b} \frac{1}{n}\sum_{i=1}^n (w^Tx_i + b - y_i)^2\] <p>为了找到最佳的 $w,b$，我们会使用梯度下降法。</p> <h2 id="梯度下降法-gradient-descent">梯度下降法 (Gradient Descent)</h2> <p>为了使这个表达式达到最小值，我们对其求梯度（gradient）：</p> \[\begin{aligned} \frac{\partial L(w,b)}{\partial w} &amp;= -\sum_{i \in [n]}2(y_i - w^Tx_i - b)\cdot \frac{\partial (w^Tx_i)}{\partial w} \\ &amp;= -\sum_{i \in [n]}2x_i(y_i - w^Tx_i - b) \\ \frac{\partial L(w,b)}{\partial b} &amp;= -\sum_{i \in [n]}2(y_i - w^Tx_i - b) \end{aligned}\] <blockquote> <p>常见矩阵/向量运算的求导</p> <p>常见的公式：</p> <ul> <li>$\displaystyle \frac{\partial x^Tx}{\partial x} = 2x$</li> <li>$\displaystyle \frac{\partial x^TAx}{\partial x} = (A + A^T)x$</li> <li>$\displaystyle \frac{\partial a^Tx}{\partial x} = a$</li> </ul> </blockquote> <p>更多可以查阅 <em>Matrix Cookbook</em>。</p> <p>实际上，对于任意的矩阵求导，我们只需要对其在每一个维度上进行讨论即可，比如推导：</p> \[\frac{\partial a^Tx}{\partial x} = \begin{pmatrix} \frac{\partial a^Tx}{\partial x_1} \\ \frac{\partial a^Tx}{\partial x_2} \\ \vdots \\ \frac{\partial a^Tx}{\partial x_d} \end{pmatrix} = \begin{pmatrix} \frac{\partial (a_1x_1 + a_2x_2 + \cdots + a_dx_d)}{\partial x_1} \\ \vdots \\ \frac{\partial (a_1x_1 + a_2x_2 + \cdots + a_dx_d)}{\partial x_d} \end{pmatrix} = \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_d \end{pmatrix} = a\] <p>有了梯度，我们就可以使用梯度下降法来更新 $w,b$ 了：</p> \[\begin{aligned} w &amp;\leftarrow w - \alpha \frac{\partial L(w,b)}{\partial w} \\ b &amp;\leftarrow b - \alpha \frac{\partial L(w,b)}{\partial b} \end{aligned}\] <p>其中 $\alpha$ 是学习率（learning rate），控制每次更新的步长,是一个大于0的超参数（hyper parameter）。</p> <p>梯度下降的终止条件：</p> <ul> <li>达到最大迭代次数</li> <li>损失函数的变化小于某个阈值 \(\|w' - w\| &lt; \text{threshold}\)</li> </ul> <h2 id="线性回归问题的闭式解讨论">线性回归问题的闭式解讨论</h2> <p>我们做出如下的定义：</p> \[X = \begin{pmatrix} x_1^T &amp; 1 \\ x_2^T &amp; 1 \\ \vdots &amp; \vdots \\ x_n^T &amp; 1 \end{pmatrix} \in \mathbb{R}^{n \times (d+1)}\] \[y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} \in \mathbb{R}^n\] \[\hat{w} = \begin{pmatrix} w \\ b \end{pmatrix} \in \mathbb{R}^{d+1}\] <p>此时我们可以将平方损失函数的和写为：</p> \[L(\hat{w}) = (y-X\hat{w})^T(y-X\hat{w}) = \|y - X\hat{w}\|^2\] <p>我们对 $L(\hat{w})$ 求导：</p> \[\begin{aligned} \frac{\partial L(\hat{w})}{\partial \hat{w}} &amp;= \frac{\partial (y - X\hat{w})^T(y - X\hat w)}{\partial \hat{w}} \\ &amp;= \frac{\partial (y^Ty - y^TX\hat{w} - \hat{w}^TX^Ty + \hat{w}^TX^TX\hat{w})}{\partial \hat{w}} \\ &amp;= -2X^T y + 2X^TX\hat{w} \\ &amp;= -2X^T(y-X\hat{w}) \end{aligned}\] <p>我们令其为0，得到：</p> \[X^TX \hat{w} = X^Ty\] <p>当 $X^TX$ 为可逆矩阵时，我们可以直接得到唯一的闭式解（closed-form solution）：</p> \[\hat{w} = (X^TX)^{-1}X^Ty\] <blockquote> <p>注意：$X^TX$ 不一定总是可逆的，我们来分情况讨论</p> <ul> <li>当 $d+1 \gt n $ 时， \(\mathrm{rank}(X^TX) = \mathrm{rank}(X) \leq \min(n, d+1) = n &lt; d+1\) 但 $X^TX \in \mathbb{R}^{(d+1) \times (d+1)}$，$X^TX$ 不是满秩（not full rank）。此时 $X^TX$ 不可逆。</li> <li>当 $d+1 \leq n$ ，且 $X$的列向量线性相关（linearly dependent）时，$X^TX$ 也不是满秩的，不可逆。</li> </ul> </blockquote> <p>当 $X^TX$ 不可逆时，何时有解？</p> <p>根据线性代数的知识， $X^TX\hat{w} = X^Ty$ 无解当且仅当 $\mathrm{rank}(X^TX) &lt; \mathrm{rank}([X^TX \mid X^Ty])$。</p> <p>但是这种情况是不可能的，因为 $\mathrm{rank}(X^TX) = \mathrm{rank}(X)$，而 $\mathrm{rank}([X^TX \mid X^Ty]) = \mathrm{rank}([X \mid y])$，且 $\mathrm{rank}(X) = \mathrm{rank}([X \mid y])$（因为都是 $X$ 的线性组合）。</p> <p>为了解决 $X^TX$ 不可逆的问题，我们可以使用正则化的方法。</p> <h2 id="l2正则化和岭回归-ridge-regression">L2正则化和岭回归 (Ridge Regression)</h2> <p>我们在损失函数中加入正则化项（regularization term），此时我们的优化目标变为：</p> \[\min_{\hat{w}} \; L(\hat{w}) + \lambda \|\hat{w}\|_2^2\] <p>其中 $\lambda &gt; 0$ 是正则化参数（regularization parameter），是一个超参数。</p> <p>\(\|\hat{w}\|_2^2 = \hat{w}^T\hat{w} = \sum_{i=1}^{d+1} \hat{w}\_i^2\) 这一个正则项惩罚了一些过大的权重。</p> <p>我们将正则化后的损失函数写为：</p> \[J(\hat{w}) = L(\hat{w}) + \lambda \|\hat{w}\|_2^2 = (y - X\hat{w})^T(y - X\hat{w}) + \lambda \hat{w}^T\hat{w}\] <p>我们对 $J(\hat{w})$ 求导:</p> \[\begin{aligned} \frac{\partial J(\hat{w})}{\partial \hat{w}} &amp;= \frac{\partial L(\hat{w})}{\partial \hat{w}} + \lambda \frac{\partial \|\hat{w}\|_2^2}{\partial \hat{w}} \\ &amp;= -2X^T(y - X\hat{w}) + 2\lambda \hat{w} \end{aligned}\] <p>令其为0，得到：</p> \[(X^TX+ \lambda I ) \hat{w} = X^Ty\] <p>此时，$X^TX + \lambda I$ 一定是可逆的（因为 $\lambda &gt; 0$），所以我们可以得到唯一的闭式解。</p> <blockquote> <p>讨论为什么 $X^TX + \lambda I$ 一定是可逆的： 设 $A = X^TX$，则 $A$ 是半正定矩阵（positive semi-definite matrix），即对于任意非零向量 $z$，都有 $z^TAz \geq 0$。 设 $B = A + \lambda I$，其中 $\lambda &gt; 0$，则对于任意非零向量 $z$，都有：</p> \[z^TBz = z^TAz + \lambda z^Tz \geq \lambda z^Tz &gt; 0\] <p>因为 $z^Tz &gt; 0$（当 $z$ 非零时）。这表明 $B$ 是正定矩阵（positive definite matrix）。 正定矩阵一定是可逆的，因此 $X^TX + \lambda I$ 一定是可逆的。</p> </blockquote> <p>事实上，加入正则项不但能保证有唯一解，还能防止过拟合（overfitting）。</p> <p>$X^TX$是实对称矩阵，因此我们可以对其进行特征值分解（eigen decomposition）：</p> \[X^TX = Q\Lambda Q^T\] <p>其中 $Q$ 是正交矩阵（orthogonal matrix），$\Lambda$ 是对角矩阵（diagonal matrix），其对角线上的元素为 $X^TX$的特征值（eigenvalues）。因此$(X^TX)^{-1}$可以写为：</p> \[(X^TX)^{-1} = Q\Lambda^{-1}Q^T = Q \begin{pmatrix} \frac{1}{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \frac{1}{\lambda_2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \frac{1}{\lambda_{d+1}} \end{pmatrix}Q^T\] <p>如果某个特征值 $\lambda_i$ 非常小，那么 $\frac{1}{\lambda_i}$ 会非常大，这会导致 $(X^TX)^{-1}$ 的值变得非常大，从而使得 $\hat{w} = (X^TX)^{-1}X^Ty$ 变得不稳定，容易受到噪声的影响，导致过拟合。</p> <p>而加入了正则项之后，不但可以防止某个$\lambda$为0导致矩阵没有逆（此时确定出唯一解），还能防止某个$\lambda$非常小导致的过拟合问题。</p> <h2 id="l1正则化和lasso回归-lasso-regression">L1正则化和Lasso回归 (Lasso Regression)</h2> <p>我们也可以使用L1正则化：</p> \[\min_{\hat{w}} \; L(\hat{w}) + \lambda \|\hat{w}|_1\] <p>其中 $|\hat{w}|<em>1 = \sum</em>{i=1}^{d+1} \lvert \hat{w}_i \rvert$。 L1正则化的一个重要性质是它倾向于产生稀疏解（sparse solution），即许多权重会被压缩为零。这在特征选择（feature selection）中非常有用，因为它可以帮助我们识别出最重要的特征。</p> <p>理解L1正则化为什么会产生稀疏解，可以从几何角度来考虑。L1正则化对应的约束区域是一个菱形，而L2正则化对应的约束区域是一个圆。当我们在损失函数的等高线上寻找最优解时，L1正则化更有可能在菱形的顶点处与等高线相切，而这些顶点通常对应于一些权重为零的解。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="flex: 0 0 60%; max-width: 60%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/notes_img/ML/L1-480.webp 480w,/assets/img/notes_img/ML/L1-800.webp 800w,/assets/img/notes_img/ML/L1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/notes_img/ML/L1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ML/">notes of ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofvci/">notes of VCI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofaip/">notes of AIP</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_aimath/">notes of AI Math Fundamentals</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ICS/">notes of ICS</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 stibiums liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?58fdb075e5aa03fbb8617845abde746c"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>