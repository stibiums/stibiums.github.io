<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 人工智能中的编程 - 第9章: 自动微分（Automatic Differentiation） | STIBIUMS_WEB </title> <meta name="author" content="stibiums liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/touxiang.jpg?68b4199d95528c9129ff55a104244865"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stibiums.github.io/notes/AIP-ch09/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> STIBIUMS_WEB </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">人工智能中的编程 - 第9章: 自动微分（Automatic Differentiation）</h1> <p class="post-meta"> Created on October 25, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> notes   <i class="fa-solid fa-hashtag fa-sm"></i> AIP   <i class="fa-solid fa-hashtag fa-sm"></i> automatic-differentiation   ·   <i class="fa-solid fa-tag fa-sm"></i> AIP </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="自动微分在-ai-框架中的地位">自动微分在 AI 框架中的地位</h2> <p>自动微分（Automatic Differentiation, AutoDiff）是深度学习框架的核心功能之一。它解决了高效计算神经网络梯度的问题，是反向传播算法（backpropagation）的高级实现方式。</p> <h3 id="ai-框架的六大任务">AI 框架的六大任务</h3> <p>在 AI 框架中，自动微分与其他组件密切配合：</p> <ol> <li> <strong>神经网络编程</strong>（NN Programming）：CNN, RNN, GraphNet, Transformer, INR 等</li> <li> <strong>自动微分</strong>（AutoDiff）：计算梯度的自动化</li> <li> <strong>数据管理与处理</strong>（Data Management）：训练、验证、测试数据集和模型参数</li> <li> <strong>模型训练与部署</strong>（Model Training &amp; Deployment）：SGD 等优化算法和部署工具</li> <li> <strong>硬件加速</strong>（Hardware Acceleration）：GPU, TPU, NPU 等</li> <li> <strong>分布式执行</strong>（Distributed Execution）：跨机器、跨 GPU 的并行计算</li> </ol> <h3 id="ai-框架的职责">AI 框架的职责</h3> <p>现代 AI 框架（如 PyTorch、TensorFlow、JAX）需要：</p> <p><strong>灵活的编程模型：</strong></p> <ul> <li>提供直观的神经网络构建接口</li> <li>自动推导计算图</li> <li>自动计算微分（梯度）</li> <li>自动优化网络</li> </ul> <p><strong>高效的计算能力：</strong></p> <ul> <li>自动编译和优化算法（子表达式消除、内核融合、内存优化）</li> <li>根据硬件自动并行化和分布式化</li> <li>支持多种处理器（CPU、GPU、TPU）</li> </ul> <h2 id="微分方法对比">微分方法对比</h2> <h3 id="三种微分方法">三种微分方法</h3> <p>计算函数导数有三种主要方法，各有优缺点：</p> <h4 id="1-符号微分symbolic-differentiation">1. 符号微分（Symbolic Differentiation）</h4> <p><strong>原理</strong>：使用数学符号操作和求导规则自动推导导数表达式</p> <p><strong>优点：</strong></p> <ul> <li>结果精确，无舍入误差</li> <li>适合简单、结构固定的表达式</li> </ul> <p><strong>缺点：</strong></p> <ul> <li>表达式爆炸：导数表达式可能比原函数复杂百倍</li> <li>效率低，执行速度慢</li> <li>难以处理控制流（if/while 等）</li> <li>对于复杂的函数或神经网络不实用</li> </ul> <p><strong>应用：</strong></p> <ul> <li>Mathematica、Maple 等计算机代数系统</li> <li>不适合深度学习框架</li> </ul> <h4 id="2-数值微分numerical-differentiation">2. 数值微分（Numerical Differentiation）</h4> <p><strong>原理</strong>：利用导数的定义近似计算</p> <p>一阶前向差分： \(f'(x) \approx \frac{f(x+h) - f(x)}{h}\)</p> <p>更精确的中心差分（二阶精度）： \(f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}\)</p> <p><strong>误差分析：</strong></p> <p>对于前向差分，截断误差为 \(O(h)$，舍入误差为\)O(\epsilon/h)\(（其中\)\epsilon\(是机器精度）。总误差为：\)E(h) = \frac{Mh}{2} + \frac{\epsilon}{h}$$</p> <p>最优步长为 \(h^* = 2\sqrt{\epsilon/M}$。对于双精度浮点数（\)\epsilon \approx 10^{-16}\(），最优步长约为\)10^{-8}$$。</p> <p><strong>具体例子：</strong></p> <p>计算 \(f(x) = x^2\) 在 \(x=2\) 处的导数（精确值为 4）：</p> <ul> <li>使用 \(h=0.01\)：\(f'(2) \approx \frac{(2.01)^2 - 2^2}{0.01} = \frac{4.0401 - 4}{0.01} = 4.01\)，误差 \(0.01\)</li> <li>使用 \(h=10^{-8}\)（中心差分）：\(f'(2) \approx \frac{(2+10^{-8})^2 - (2-10^{-8})^2}{2 \times 10^{-8}} \approx 4.000000\)，误差 \(\sim 10^{-8}\)</li> </ul> <p><strong>优点：</strong></p> <ul> <li>实现简单，易于理解</li> <li>对任何黑盒函数都适用</li> </ul> <p><strong>缺点：</strong></p> <ul> <li> <strong>精度差</strong>：在实际计算中，由于舍入误差和截断误差的权衡，精度难以提高</li> <li> <strong>计算复杂度高</strong>：对于 \(n\) 维输入需要 \(n+1\) 次函数调用（或 \(2n\) 次用中心差分）</li> <li> <strong>不可行性</strong>：对于现代神经网络（百万级参数），需要百万次前向计算</li> <li> <strong>步长选择困难</strong>：需要仔细调整步长 \(h\)，不同函数最优值不同</li> <li> <strong>数值不稳定</strong>：在非光滑函数处表现差</li> </ul> <p><strong>应用：</strong></p> <ul> <li> <strong>梯度检验（Gradient Checking）</strong>：验证自动微分实现的正确性 <ul> <li>计算数值梯度和自动微分梯度，比较两者差异</li> <li>相对误差应小于 \(10^{-7}\)（对于中心差分）</li> </ul> </li> <li> <strong>简单场景</strong>：黑盒优化问题</li> </ul> <p><strong>梯度检验示例代码：</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">计算 f 在 x 处的数值梯度</span><span class="sh">"""</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="n">x_h</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">copy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">h</span>
        <span class="n">fxh_pos</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x_h</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>

        <span class="n">x_h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">h</span>
        <span class="n">fxh_neg</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x_h</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>

        <span class="n">grad</span><span class="p">.</span><span class="n">flat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh_pos</span> <span class="o">-</span> <span class="n">fxh_neg</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad</span>

<span class="c1"># 验证自动微分
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">y</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">auto_grad</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># 计算数值梯度
</span><span class="k">def</span> <span class="nf">f_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">num_grad</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f_numpy</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

<span class="c1"># 检查相对误差
</span><span class="n">rel_error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">auto_grad</span> <span class="o">-</span> <span class="n">num_grad</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">auto_grad</span> <span class="o">+</span> <span class="n">num_grad</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">相对误差: </span><span class="si">{</span><span class="n">rel_error</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># 应该 &lt; 1e-7
</span></code></pre></div></div> <h4 id="3-自动微分automatic-differentiation">3. 自动微分（Automatic Differentiation）</h4> <p><strong>原理</strong>：将函数分解为基本操作，逐步应用链式法则</p> <p>根据计算顺序分为：</p> <ul> <li> <strong>前向模式</strong>（Forward Mode）：从输入到输出</li> <li> <strong>反向模式</strong>（Reverse Mode）：从输出到输入</li> </ul> <p><strong>优点：</strong></p> <ul> <li> <strong>精度高</strong>：只有浮点舍入误差</li> <li> <strong>效率高</strong>：\(O(1)\) 到 \(O(n)\) 次基本操作</li> <li>支持控制流（if/while）</li> <li>适合深度学习</li> </ul> <p><strong>缺点：</strong></p> <ul> <li>实现复杂度较高</li> <li>需要维护计算图或中间变量</li> </ul> <p><strong>应用：</strong></p> <ul> <li>深度学习框架（PyTorch、TensorFlow）</li> <li>科学计算库（JAX）</li> </ul> <h3 id="方法对比表">方法对比表</h3> <table> <thead> <tr> <th>特性</th> <th>符号微分</th> <th>数值微分</th> <th>自动微分</th> </tr> </thead> <tbody> <tr> <td>精度</td> <td>精确</td> <td>近似（低）</td> <td>精确（舍入误差）</td> </tr> <tr> <td>复杂度</td> <td>\(O(n)\) 个导数表达式</td> <td>\(O(n)\) 次函数调用</td> <td>\(O(n)\) 个算子</td> </tr> <tr> <td>速度</td> <td>慢（表达式爆炸）</td> <td>慢（多次计算）</td> <td>快</td> </tr> <tr> <td>控制流支持</td> <td>困难</td> <td>支持</td> <td>支持</td> </tr> <tr> <td>实现复杂度</td> <td>高</td> <td>低</td> <td>中等</td> </tr> <tr> <td>适用范围</td> <td>简单表达式</td> <td>黑盒函数</td> <td>复杂函数/神经网络</td> </tr> </tbody> </table> <h2 id="自动微分的基础">自动微分的基础</h2> <h3 id="链式法则chain-rule">链式法则（Chain Rule）</h3> <p>自动微分的数学基础是链式法则。对于复合函数 \(y = f(g(x))\)：</p> \[\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}\] <p>其中 \(u = g(x)\)</p> <p>对于多元函数，偏导数的链式法则为：</p> \[\frac{\partial y}{\partial x_i} = \sum_j \frac{\partial y}{\partial u_j} \cdot \frac{\partial u_j}{\partial x_i}\] <p><strong>具体例子：</strong></p> <p>考虑函数 \(y = \sin(x^2)\)，我们要计算 $$\frac{dy}{dx}$：</p> <p>设 \(u = x^2\)，则 \(y = \sin(u)\)</p> \[\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} = \cos(u) \cdot 2x = 2x \cos(x^2)\] <table> <tbody> <tr> <td>在 \(x = 1\) 处：$$\frac{dy}{dx}</td> <td>_{x=1} = 2 \cos(1) \approx 2 \times 0.5403 \approx 1.0806$$</td> </tr> </tbody> </table> <p><strong>多元函数例子：</strong></p> <p>对于 \(z = (x + y)^2\)，求 \(\frac{\partial z}{\partial x}$ 和\)\frac{\partial z}{\partial y}$$：</p> <p>设 \(u = x + y\)，则 \(z = u^2\)</p> \[\frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} \cdot \frac{\partial u}{\partial x} = 2u \cdot 1 = 2(x + y)\] \[\frac{\partial z}{\partial y} = \frac{\partial z}{\partial u} \cdot \frac{\partial u}{\partial y} = 2u \cdot 1 = 2(x + y)\] <h3 id="计算图computational-graph">计算图（Computational Graph）</h3> <p>函数的计算可以表示为<strong>计算图</strong>，其中：</p> <ul> <li> <strong>节点</strong>表示基本操作（加、乘、激活函数等）</li> <li> <strong>边</strong>表示数据流动</li> <li> <strong>叶子节点</strong>表示输入变量</li> <li> <strong>根节点</strong>表示输出（损失函数）</li> </ul> <p>例如，\(y = (x_1 + x_2) \times (x_1 - x_2)\) 的计算图：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      x₁  x₂
       \ /
    (加法)  (减法)
        \    /
      (乘法)
         |
         y
</code></pre></div></div> <h3 id="前向模式forward-mode-ad">前向模式（Forward Mode AD）</h3> <p><strong>思想</strong>：从输入开始，沿着计算图正向传播，逐层计算导数（也称 Tangent Linear）</p> <p><strong>详细过程：</strong></p> <ol> <li>初始化：对要求导的输入 \(x_i\)，设 \(\dot{x}_i = 1\)（其他输入的导数为 0）</li> <li>前向传播：对计算图中每个中间节点 \(u_j\)，计算： \(\dot{u}_j = \sum_k \frac{\partial u_j}{\partial u_k} \dot{u}_k\)</li> <li>最后得到输出的导数：\(\dot{y} = \frac{dy}{dx_i}\)</li> </ol> <p><strong>具体例子：</strong></p> <p>计算 \(y = (x_1 + x_2) \times (x_1 - x_2)\) 对 \(x_1\) 的导数，在 \(x_1 = 3, x_2 = 2\) 处：</p> <p>计算过程：</p> <ul> <li>\(u_1 = x_1 + x_2 = 5\)，\(\dot{u}_1 = \dot{x}_1 + \dot{x}_2 = 1 + 0 = 1\)</li> <li>\(u_2 = x_1 - x_2 = 1\)，\(\dot{u}_2 = \dot{x}_1 - \dot{x}_2 = 1 - 0 = 1\)</li> <li>\(y = u_1 \times u_2 = 5\)，\(\dot{y} = u_2 \cdot \dot{u}_1 + u_1 \cdot \dot{u}_2 = 1 \times 1 + 5 \times 1 = 6\)</li> </ul> <p>验证：\(\frac{\partial y}{\partial x_1} = \frac{\partial}{\partial x_1}[(x_1+x_2)(x_1-x_2)] = (x_1-x_2) + (x_1+x_2) = 2x_1 = 6\) ✓</p> <p><strong>计算量分析：</strong></p> <p>对于 \(n\) 个输入、1 个输出，需要 \(n\) 次前向传播（每次对应一个输入变量）</p> <p>若计算图有 \(m\) 条边，总复杂度为 \(O(n \times m)\)</p> <p><strong>适用场景：</strong></p> <ul> <li>输入少、输出多的函数</li> <li>例如：神经网络中的向量场计算，梯度方向场等</li> </ul> <h3 id="反向模式reverse-mode-ad">反向模式（Reverse Mode AD）</h3> <p><strong>思想</strong>：从输出开始，沿着计算图逆向传播，逐层反向计算导数（也称 Adjoint 或 Cotangent Linear）</p> <p><strong>详细过程：</strong></p> <ol> <li> <strong>前向阶段</strong>：计算函数值，保存所有中间结果</li> <li> <strong>初始化</strong>：\(\bar{y} = \frac{\partial L}{\partial y} = 1\)（对于标量输出）</li> <li> <strong>反向传播</strong>：从输出向输入逐层计算伴随变量（adjoint）： \(\bar{u}_j = \sum_k \bar{u}_k \frac{\partial u_k}{\partial u_j}\)</li> <li> <strong>最后得到</strong>：\(\bar{x}_i = \frac{\partial L}{\partial x_i}\)</li> </ol> <p><strong>具体例子：</strong></p> <p>同上，计算 \(y = (x_1 + x_2) \times (x_1 - x_2)\) 对 \(x_1, x_2\) 的导数：</p> <p>前向阶段：</p> <ul> <li> \[u_1 = x_1 + x_2 = 5\] </li> <li> \[u_2 = x_1 - x_2 = 1\] </li> <li> \[y = u_1 \times u_2 = 5\] </li> </ul> <p>反向阶段：</p> <ul> <li> \[\bar{y} = 1\] </li> <li> \[\bar{u}_1 = \bar{y} \cdot \frac{\partial y}{\partial u_1} = 1 \times u_2 = 1\] </li> <li> \[\bar{u}_2 = \bar{y} \cdot \frac{\partial y}{\partial u_2} = 1 \times u_1 = 5\] </li> <li> \[\bar{x}_1 = \bar{u}_1 \cdot \frac{\partial u_1}{\partial x_1} + \bar{u}_2 \cdot \frac{\partial u_2}{\partial x_1} = 1 \times 1 + 5 \times 1 = 6\] </li> <li> \[\bar{x}_2 = \bar{u}_1 \cdot \frac{\partial u_1}{\partial x_2} + \bar{u}_2 \cdot \frac{\partial u_2}{\partial x_2} = 1 \times 1 + 5 \times (-1) = -4\] </li> </ul> <p>验证：\(\frac{\partial y}{\partial x_2} = 2x_2 = 4$... 不对，让我重算：\)y = x_1^2 - x_2^2\(，\)\frac{\partial y}{\partial x_2} = -2x_2 = -4$$ ✓</p> <p><strong>计算量分析：</strong></p> <p>无论有多少个输入，反向模式只需 1 次前向 + 1 次反向，总复杂度为 \(O(m)\)（其中 \(m\) 为计算图的边数）</p> <p><strong>内存成本：</strong></p> <p>需要保存前向计算的所有中间结果，内存复杂度为 $$O(m + n)$</p> <p><strong>适用场景：</strong></p> <ul> <li> <strong>神经网络训练</strong>：输入参数多（百万级），输出少（标量损失）</li> <li><strong>这是深度学习框架中使用反向传播的根本原因</strong></li> </ul> <p><strong>反向模式 vs 前向模式的效率对比：</strong></p> <p>对于有 \(n = 10^6\) 个参数、单个标量输出的神经网络：</p> <ul> <li> <strong>前向模式</strong>：需要 \(10^6\) 次前向传播 → 总计 \(10^6\) 倍的计算量</li> <li> <strong>反向模式</strong>：1 次前向 + 1 次反向 → 总计 2 倍的计算量</li> </ul> <p><strong>性能比率</strong>：\(10^6 : 2 \approx 500000 : 1\)，这说明反向模式快 50 万倍！</p> <h2 id="深度学习中的自动微分">深度学习中的自动微分</h2> <h3 id="反向传播backpropagation">反向传播（Backpropagation）</h3> <p>深度学习使用反向模式自动微分来高效计算损失函数对所有参数的梯度。</p> <p><strong>过程：</strong></p> <ol> <li> <strong>前向传播</strong>：计算损失 \(L = f(x, \theta)\)</li> <li> <strong>初始化</strong>：\(\bar{L} = \frac{\partial L}{\partial L} = 1\)</li> <li> <strong>反向传播</strong>：逐层计算 \(\bar{\theta}^{(l)} = \frac{\partial L}{\partial \theta^{(l)}}\)</li> <li> <strong>参数更新</strong>：\(\theta \leftarrow \theta - \eta \bar{\theta}\)</li> </ol> <h3 id="计算效率分析">计算效率分析</h3> <p>对于有 \(n\) 个参数的神经网络：</p> <ul> <li> <strong>数值微分</strong>：需要 \(n+1\) 次前向传播，复杂度 \(O(n)\)</li> <li> <strong>反向传播</strong>：只需 1 次前向 + 1 次反向传播，复杂度 \(O(1)\)</li> </ul> <p>这是反向传播相比数值微分的关键优势。</p> <h2 id="面向对象实现">面向对象实现</h2> <h3 id="操作算子的包装">操作算子的包装</h3> <p>在自动微分框架中，每个基本操作都需要包装为操作对象，包含：</p> <p><strong>前向计算</strong>：\(y = f(x_1, \ldots, x_n)\)</p> <p><strong>反向梯度计算</strong>：\(\bar{x}_i = \bar{y} \cdot \frac{\partial y}{\partial x_i}\)</p> <h3 id="计算图的构建">计算图的构建</h3> <p><strong>动态图</strong>（Dynamic Graph，如 PyTorch）：</p> <ul> <li>在代码执行时动态构建计算图</li> <li>灵活支持控制流</li> <li>调试友好</li> </ul> <p><strong>静态图</strong>（Static Graph，如 TensorFlow 早期）：</p> <ul> <li>先定义计算图，再执行</li> <li>优化空间大</li> <li>部署效率高</li> </ul> <p>现代框架（PyTorch 的 eager execution、TensorFlow 的 eager mode）倾向于动态图。</p> <h2 id="常见的自动微分库">常见的自动微分库</h2> <h3 id="pytorch">PyTorch</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>

<span class="n">y</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>  <span class="c1"># 自动计算梯度
</span><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># 输出: tensor([4., 6.])
</span></code></pre></div></div> <p><strong>特点：</strong></p> <ul> <li>命令式编程，易于学习</li> <li>动态计算图</li> <li>研究友好</li> </ul> <h3 id="tensorflow">TensorFlow</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>  <span class="c1"># 输出: [4. 6.]
</span></code></pre></div></div> <p><strong>特点：</strong></p> <ul> <li>支持声明式和命令式编程</li> <li>静态优化与动态灵活性的平衡</li> <li>生产部署强大</li> </ul> <h3 id="jax">JAX</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">grad_f</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># 输出: [4. 6.]
</span></code></pre></div></div> <p><strong>特点：</strong></p> <ul> <li>函数式编程风格</li> <li>可组合的转换（grad、vmap、jit）</li> <li>适合研究创新</li> </ul> <h2 id="总结">总结</h2> <p>自动微分是 AI 框架的核心，通过高效的链式法则计算梯度。相比数值微分的低精度和高计算量，自动微分提供了：</p> <ol> <li> <strong>精确性</strong>：只有浮点舍入误差</li> <li> <strong>效率</strong>：反向模式对神经网络训练最优（\(O(1)\) 倍计算）</li> <li> <strong>灵活性</strong>：支持动态控制流</li> <li> <strong>可扩展性</strong>：支持复杂的神经网络架构</li> </ol> <p>现代深度学习框架通过包装基本操作算子，自动构建和执行计算图，为用户屏蔽了自动微分的复杂性，使得写神经网络代码就像写普通 Python 代码一样简单。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ML/">notes of ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofvci/">notes of VCI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesofaip/">notes of AIP</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_aimath/">notes of AI Math Fundamentals</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/09/08/notesof_ICS/">notes of ICS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 stibiums liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 25, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?58fdb075e5aa03fbb8617845abde746c"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>